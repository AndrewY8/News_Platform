{
  "tavily_Technology News": {
    "success": true,
    "urls_found": 3,
    "urls_scraped": 3,
    "retrieval_time": 1.2486398220062256,
    "scraping_time": 0.5281164646148682,
    "total_time": 1.7767562866210938,
    "content_analysis": {
      "successful_scrapes": 3,
      "failed_scrapes": 0,
      "total_content_length": 40806,
      "total_images": 21,
      "relevance_score": 100.0,
      "content_samples": [
        {
          "url": "https://blog.google/technology/ai/2024-ai-extraordinary-progress-advancement/",
          "title": "Year in review: Google's biggest AI advancements of 2024",
          "content_length": 25929,
          "keyword_matches": 4,
          "sample_content": "Year in review: Google's biggest AI advancements of 2024\n2024: A year of extraordinary progress and advancement in AI\nJan 23, 2025\n\u00b7\nShare\nTwitter\nFacebook\nLinkedIn\nMail\nCopy link\nA look back on a yea..."
        },
        {
          "url": "https://blog.google/technology/ai/google-ai-big-scientific-breakthroughs-2024/",
          "title": "How Google AI is advancing science",
          "content_length": 10007,
          "keyword_matches": 4,
          "sample_content": "How Google AI is advancing science\n9 ways AI is advancing science\nNov 18, 2024\n\u00b7\nShare\nTwitter\nFacebook\nLinkedIn\nMail\nCopy link\nWe\u2019re sharing a recap of some of the biggest scientific breakthroughs in..."
        },
        {
          "url": "https://www.forbes.com/sites/bernardmarr/2024/12/16/6-game-changing-ai-breakthroughs-that-defined-2024/",
          "title": "6 Game-Changing AI Breakthroughs That Defined 2024",
          "content_length": 4870,
          "keyword_matches": 3,
          "sample_content": "6 Game-Changing AI Breakthroughs That Defined 2024\nInnovation\nEnterprise Tech\n6 Game-Changing AI Breakthroughs That Defined 2024\nBy\nBernard Marr\n,\nContributor.\nFollow Author\nDec 16, 2024, 02:09am EST\n..."
        }
      ],
      "titles": [
        "Year in review: Google's biggest AI advancements of 2024",
        "How Google AI is advancing science",
        "6 Game-Changing AI Breakthroughs That Defined 2024"
      ]
    },
    "search_results": [
      {
        "href": "https://blog.google/technology/ai/2024-ai-extraordinary-progress-advancement/",
        "body": "At the start of 2024, we introduced ImageFX, a new generative AI tool that creates images from text prompts, and MusicFX, a tool for creating up"
      },
      {
        "href": "https://blog.google/technology/ai/google-ai-big-scientific-breakthroughs-2024/",
        "body": "In 2024, Google Research partnered with the U.S. Forest Service to develop FireSat, an AI model and new global satellite constellation designed"
      },
      {
        "href": "https://www.forbes.com/sites/bernardmarr/2024/12/16/6-game-changing-ai-breakthroughs-that-defined-2024/",
        "body": "Here, we explore seven pivotal AI developments, including historic regulatory frameworks and Nobel Prize-winning breakthroughs, that are"
      }
    ],
    "scraped_results": [
      {
        "url": "https://blog.google/technology/ai/2024-ai-extraordinary-progress-advancement/",
        "raw_content": "Year in review: Google's biggest AI advancements of 2024\n2024: A year of extraordinary progress and advancement in AI\nJan 23, 2025\n\u00b7\nShare\nTwitter\nFacebook\nLinkedIn\nMail\nCopy link\nA look back on a year of breakthroughs, progress and extraordinary accomplishments.\nDemis Hassabis\nCEO Google DeepMind\nJames Manyika\nSenior Vice President, Research, Technology & Society\nJeff Dean\nChief Scientist\nRead AI-generated summary\nBullet points\nThis article summarizes Google's AI advancements in 2024, highlighting their commitment to responsible development.\nGoogle released Gemini 2.0, a powerful AI model designed for the \"agentic era,\" and integrated it into various products.\nThey made significant progress in generative AI, releasing updates to Imagen, Veo, and MusicFX, empowering creativity.\nGoogle also advanced robotics, hardware, and computing, with breakthroughs in quantum computing and chip design.\nThey explored AI's potential in science, biology, and mathematics, with notable achievements in protein structure prediction and geometry.\nSummaries were generated by Google AI. Generative AI is experimental.\nShare\nTwitter\nFacebook\nLinkedIn\nMail\nCopy link\nAs we move into 2025, we wanted to take a moment to recognize the astonishing progress of the last year. From\nnew Gemini models built for the agentic era\nand empowering\ncreativity\n, to an\nAI system\nthat designs novel, high-strength protein binders,\nAI\u2013enabled neuroscience\nand even\nlandmark advances\nin quantum computing, we\u2019ve been boldly and responsibly advancing the frontiers of artificial intelligence and all the ways it can benefit humanity.\nAs we and our colleagues\nwrote\ntwo years ago in an essay titled\nWhy we focus on AI\n:\n\u201c\nOur approach to developing and harnessing the potential of AI is grounded in our founding mission \u2014 to organize the world\u2019s information and make it universally accessible and useful \u2014 and it is shaped by our commitment to improve the lives of as many people as possible\n.\u201d\nThis remains as true today as it was when we first wrote it.\nIn this 2024 Year-in-Review post, we look back on a year's worth of extraordinary progress in AI, made possible by the many incredible teams across Google, that helped deliver on that mission and commitment \u2014 progress that sets the stage for more to come this year.\nRelentless innovation in models, products and technologies\n2024 was a year of experimenting, fast shipping, and putting our latest technologies in the hands of developers.\nIn December 2024, we released the first models in our\nGemini 2.0\nexperimental series \u2014 AI models designed for the agentic era. First out of the gate was Gemini 2.0 Flash, our workhorse model, followed by prototypes from the frontiers of our agentic research including: an updated\nProject Astra\n, which explores the capabilities of a universal AI assistant;\nProject Mariner\n, an early prototype capable of taking actions in Chrome as an experimental extension; and\nJules\n, an AI-powered code agent. We're looking forward to bringing Gemini 2.0\u2019s powerful capabilities to our flagship products \u2014 in Search, we\u2019ve already started testing in\nAI Overviews\n, which are now used by over a billion people to ask new types of questions.\nWe also released\nDeep Research\n, a new agentic feature in Gemini Advanced that saves people hours of research work by creating and executing multi-step plans for finding answers to complicated questions; and introduced\nGemini 2.0 Flash Thinking Experimental\n, an experimental model that explicitly shows its thoughts.\nThese advances followed swift progress earlier in the year, from incorporating\nGemini\u2019s capabilities into more Google products\nto the release of\nGemini 1.5 Pro\nand\nGemini 1.5 Flash\n\u2014 a model optimized for speed and efficiency. 1.5 Flash\u2019s compact size made it more cost-efficient to serve, and in 2024 it became our most popular model for developers.\nAnd we improved and updated\nAI Studio\n, which provides a host of resources for developers. It is now available as a progressive web app (PWA) that can be installed on desktop, iOS and Android.\nNotably, it\u2019s been exciting to see the public reception to several\nnew features\nfor NotebookLM, such as Audio Overviews, which can take uploaded source material and produce a\n\u201cdeep dive\u201d discussion\nbetween\ntwo AI hosts\n.\nYour browser does not support the audio element.\nNotebookLM Audio Overview\nIn this Audio Overview, two AI hosts dive into the world of NotebookLM updates.\nMore natural and intuitive handling of speech input and output remains at the core of several of our products:\nGemini Live\n,\nProject Astra\n,\nJourney Voices\nand\nYouTube\u2019s auto dubbing\n.\nContinuing our long history of contributing innovations to the open community \u2014\u00a0such as with\nTransformers\n,\nTensorFlow\n,\nBERT\n,\nT5\n,\nJAX\n,\nAlphaFold\nand\nAlphaCode\n\u2014 we released two new models from\nGemma\n, our state-of-the-art open model built from the same research and technology used to create the Gemini models. Gemma\noutperformed\nsimilarly sized open models on capabilities like question answering, reasoning, math / science and coding. And we released\nGemma Scope\n, which provides tools that help researchers understand the inner workings of Gemma 2.\nWe also continued to improve the factuality of our models and minimize hallucinations. In December, for example, we published\nFACTS Grounding\n, a new benchmark \u2014 based on collaboration between Google DeepMind, Google Research and Kaggle \u2014 for evaluating how accurately large language models ground their responses in provided source material and avoid hallucinations.\nThe FACTS Grounding dataset comprises 1,719 examples, each carefully crafted to require long-form responses grounded in the context document provided.\nWe tested leading LLMs using FACTS Grounding, launched the\nFACTS leaderboard\non Kaggle and are proud that Gemini 2.0 Flash Experimental, Gemini 1.5 Flash and Gemini 1.5 Pro currently have the three highest factuality scores, with gemini-2.0-flash-exp at 83.6%.\nMoreover, we improved underlying ML efficiency through pioneering\ntechniques\nlike\nblockwise parallel decoding\n,\nimproved confidence-based deferral\nand\nspeculative decoding\nthat reduce the inference times of LLMs, allowing them to generate responses more quickly. These improvements are used across Google products and set a standard throughout the industry.\nCombining AI with sport, in March we released\nTacticAI\n, an AI system for football tactics that can provide experts with tactical insights, particularly on corner kicks.\nUnderlying all of our models and products is our ongoing commitment to research leadership. Indeed, in a\n2010-2023 WIPO survey of citations for papers on Generative AI\n, Google including Google Research and Google DeepMind\u2019s citations were more than double the second-most cited institution.\nThis WIPO graph, based on January 2024 data from The Lens, illustrates more than a decade\u2019s worth of Alphabet\u2019s generative AI scientific publication efforts.\nFinally, progress was made with Project Starline, our \u201cmagic window\u201d technology project that enables friends, families and coworkers to feel like they\u2019re together from any distance. We\npartnered with HP\nto start commercialization, with the goal of enabling it directly from video conferencing services like Google Meet and Zoom.\nEmpowering creative vision with generative AI\nWe believe AI holds great potential to enable new forms of creativity, democratize creative output and help people express their artistic visions. This is why last year we introduced a series of updates across our generative media tools, covering images, music and video.\nAt the start of 2024, we\nintroduced\nImageFX, a new generative AI tool that creates images from text prompts, and MusicFX, a tool for creating up-to-70-second audio clips also based on text prompts. At I/O, we\nshared an early preview\nof MusicFX DJ, a tool that helps bring the joy of live music creation to more people. In October, we collaborated with\nJacob Collier\non making MusicFX DJ simpler to use, especially for new or aspiring musicians. And we updated our music AI toolkit\nMusic AI Sandbox\n, and evolved our\nDream Track experiment\nwhich allowed U.S. creators to explore a range of genres and prompts that generate instrumental soundtracks with powerful text-to-music models.\nLater in 2024, we released state-of-the-art updates to our image and video models:\nVeo 2\nand\nImagen 3\n. As our highest quality text-to-image model, Imagen 3 is capable of generating images with even better detail, richer lighting and fewer distracting artifacts than our previous models; while Veo demonstrated an improved understanding of real-world physics and the nuances of human movement and expression alongside its overall attention-to-detail and realism.\nVeo represents a significant step forward in high-quality video generation.\nResearch in this field continued apace. We explored ways to use AI to improve editing, for example by\nusing it\nto control of attributes like transparency, roughness or other physical properties of objects:\nIn these examples of AI editing with synthetic data generation, Input shows a novel, held-out image the model has never seen before. Output shows the model output, which successfully edits material properties.\nIn the field of\naudio generation\n, we announced improvements to video-to-audio (V2A) technology, which can generate dynamic soundscapes through natural language text prompts based on on-screen action. This technology is pairable with AI-created video through\nVeo\n.\nGames are an ideal environment for creative exploration of new worlds, as well as training and evaluating embodied agents. In 2024, we introduced\nGenie 2\n, a foundation world model capable of generating an endless variety of action-controllable, playable 3D environments for training and evaluating embodied agents. This followed the\nintroduction of SIMA\n, a Scalable Instructable Multiworld Agent that can follow natural-language instructions to carry out tasks in a variety of video game settings.\nThe architecture of intelligence: advances in robotics, hardware and computing\nAs our multimodal models become more capable and gain a better understanding of the world and its physics, they are making possible incredible new advances in robotics and bringing us closer to our goal of ever-more capable and helpful robots.\nWith ALOHA Unleashed, our robot learned to tie a shoelace, hang a shirt, repair another robot, insert a gear and even clean a kitchen.\nAt the beginning of the year, we\nintroduced\nAutoRT, SARA-RT and RT-Trajectory, extensions of our\nRobotics Transformers\nwork intended to help robots better understand and navigate their environments, and make decisions faster. We also published\nALOHA Unleashed\n, a breakthrough in teaching robots on how to use two robotic arms in coordination, and\nDemoStart\n, which uses a reinforcement learning algorithm to improve real-world performance on a multi-fingered robotic hand by using simulations.\nRobotic Transformer 2 (RT-2) is a novel vision-language-action model that learns from both web and robotics data.\nBeyond robotics, our\nAlphaChip\nreinforcement learning method for accelerating and improving chip floorplanning is transforming the design process for chips found in data centers, smartphones and more. To accelerate adoption of these techniques, we released a\npre-trained checkpoint\nto enable external parties to more easily make use of the\nAlphaChip open source release\nfor their own chip designs. And we made\nTrillium\n, our sixth-generation and most performant TPU to date, generally available to Google Cloud customers. Advances in computer chips have accelerated AI. And now, AI can return the favor.\nAlphaChip can learn the relationships between interconnected chip components and generalize across chips, letting AlphaChip improve with each layout it designs.\nOur research also focused on correcting the errors in the physical hardware of today's quantum computers. In November, we launched\nAlphaQubit\n, an AI-based decoder that identifies quantum computing errors with state-of-the-art accuracy. This collaborative work brought together Google DeepMind\u2019s ML knowledge and Google Research\u2019s error correction expertise to accelerate progress on building a reliable quantum computer. In tests, it made 6% fewer errors than tensor network methods and 30% fewer errors than correlated matching.\nThen in December, the Google Quantum AI team, part of Google Research, announced\nWillow\n, our latest quantum chip which can perform in under five minutes a benchmark computation that would take one of today\u2019s fastest supercomputers 10 septillion years. Willow can reduce errors exponentially as it scales up using more qubits. In fact, it used our quantum error correction to cut the error rate in half, solving a 30+ year challenge known in the field as \u201cbelow threshold.\u201d This leap forward won the\nPhysics Breakthrough of the Year\naward.\nWillow has state-of-the-art performance across a number of metrics.\nUncovering new solutions: progress in science, biology and mathematics\nWe continued to push the envelope on accelerating scientific progress with AI-based approaches, releasing a series of tools and papers this year that showed just how useful and powerful a tool AI is for advancing science and mathematics. We're sharing a few highlights.\nIn January, we introduced\nAlphaGeometry\n, an AI system engineered to solve complex geometry problems. Our updated version, AlphaGeometry 2, and AlphaProof, a reinforcement-learning-based system for formal math reasoning,\nachieved the same level as a silver medalist\nin July 2024\u2019s\nInternational Mathematical Olympiad\n.\nAlphaGeometry 2 solved Problem 4 in July 2024\u2019s International Mathematical Olympiad within 19 seconds after receiving its formalization. Problem 4 asked to prove the sum of \u2220KIL and \u2220XPY equals 180\u00b0.\nIn collaboration with Isomorphic Labs, we introduced\nAlphaFold 3\n, our latest model which predicts the structure and interactions of all of life\u2019s molecules. By accurately predicting the structure of proteins, DNA, RNA, ligands and more, and how they interact, we hope it will transform our understanding of the biological world and drug discovery.\nAlphaFold 3\u2019s capabilities come from its next-generation architecture and training that now covers all of life\u2019s molecules.\nWe made several key developments in protein-shaping. We announced\nAlphaProteo\n, an AI system for designing novel, high-strength protein binders. AlphaProteo can lead to the discovery of new drugs, the development of biosensors and improve our understanding of biological processes.\nAlphaProteo can generate new protein binders for diverse target proteins.\nIn collaboration with Harvard\u2019s Lichtman Lab and others, we\nproduced\na nano-scale mapping of a piece of the human brain at a level of detail never previously achieved, and made it publicly available for researchers to build on. This follows\na decade of working to advance our understanding of connectomics\n, with earlier work on fly brain and mouse brain connectomics now giving way to the larger scale and more complex human brain connectomics.\nIn the deepest layer of the cortex, clusters of cells tend to occur in mirror-image orientation to one another, as shown in this brain mapping project.\nThen in late November, as part of a\nbroader effort\nto expand and deepen public dialogue around science and AI, we co-hosted\nthe AI for Science Forum\nwith the Royal Society, which convened\nscientists\n, researchers, governmental leaders and executives to discuss\nkey topics\nlike cracking the protein structure prediction challenge, mapping the human brain and saving lives through accurate forecasting and spotting wildfires. We hosted a Q&A with the four Nobel Laureates in attendance at the forum, Sir Paul Nurse, Jennifer Doudna, Demis Hassabis and John Jumper, which is available to listen to via the Google DeepMind\npodcast\n.\nThis was also a landmark year for another reason: Demis Hassabis and John Jumper, along with David Baker, were awarded the\n2024 Nobel Prize\u00ae in Chemistry\nfor their work on AlphaFold 2. As the Nobel committee\nrecognized\n, their work:\n\"[H]as opened up completely new possibilities to design proteins that have never been seen before, and we now have access to predicted structures of all 200 million known proteins. These are truly great achievements.\"\nIt was also exciting to see the\n2024 Nobel Prize\u00ae in Physics\nawarded to recently retired long-time Googler Geoffrey Hinton (along with John Hopfield), \"for foundational discoveries and inventions that enable machine learning with artificial neural networks.\u201d\nThe Nobels followed additional recognitions for Google including the\nNeurIPS 2024 Test of Time Paper Awards\nfor\nSequence to Sequence Learning with Neural Networks\nand\nGenerative Adversarial Nets\n, and the\nBeale\u2014Orchard-Hays Prize\n, which was awarded to a collaborative team of educators and Google professionals for groundbreaking work on\nPrimal-Dual Linear Programming (PDLP)\n. (PDLP, now part of\nGoogle OR Tools\n, helps solve large-scale linear programming problems with real-world applications from\ndata center network traffic engineering\nto\ncontainer shipping optimization\n.)\nAI for the benefit of humanity\nThis year, we made a number of product advances and published research that showed how AI can benefit people directly and immediately, ranging from preventative and diagnostic medicine to disaster readiness and recovery to learning.\nIn healthcare, AI holds the promise of democratizing quality of care in key areas, such as early\ndetection of cardiovascular disease\n. Our\nresearch\ndemonstrated how using a simple fingertip device that measures variations in blood flow, combined with basic metadata, can predict heart health risks. We built on previous AI-enabled diagnostic research for tuberculosis,\ndemonstrating\nhow AI models can be used for accurate TB screenings in populations with high rates of TB and HIV. This is important to reducing the prevalence of TB (more than\n10 million people\nfall ill with it each year), as roughly 40% of people with TB go\nundiagnosed\n.\nOn the MedQA (USMLE-style) benchmark, Med-Gemini attains a new state-of-the-art score, surpassing our prior best (\nMed-PaLM 2\n) by a significant margin of 4.6%.\nOur Gemini model is a powerful tool for professionals generally, but our teams are also working to create fine-tuned models for other domains. For example, we introduced\nMed-Gemini\n, a new family of next-generation models that combine training on de-identified medical data with Gemini\u2019s reasoning, multimodal and long-context abilities. On the MedQA US Medical Licensing Exam (USMLE)-style question benchmark, Med-Gemini\nachieves\na state-of-the-art performance of 91.1% accuracy, surpassing our prior best of Med-PaLM 2 by 4.6% (shown above).\nWe are exploring how machine learning can help medical fields struggling with access to imaging expertise, such as\nradiology, dermatology and pathology\n. In the past year, we\nreleased\ntwo research tools,\nDerm Foundation\nand\nPath Foundation\n, that can help develop models for diagnostic tasks, image indexing and curation and biomarker discovery and validation. We collaborated with physicians at Stanford Medicine on an open-access, inclusive\nSkin Condition Image Network (SCIN) dataset\n. And we unveiled\nCT Foundation\n, a medical imaging embedding tool used for rapidly training models for research.\nWith regard to learning, we explored new generative AI tools to support educators and learners. We introduced\nLearnLM\n, our new family of models fine-tuned for learning and used it to enhance learning experiences in products like Search, YouTube and Gemini; a recent report showed LearnLM\noutperformed\nother leading AI models. We also\nmade it available\nto developers as an experimental model in AI Studio. Our new conversational learning companion,\nLearnAbout\n, uses AI to help you dive deeper into any topic you\u2019re curious about, while\nIlluminate\nlets you turn content into engaging AI-generated audio discussions.\nIn the fields of disaster forecasting and preparedness, we announced several breakthroughs. We introduced\nGenCast\n, our new high-resolution AI ensemble model, which improves day-to-day weather and extreme events forecasting across all possible weather trajectories. We also introduced our\nNeuralGCM model\n, able to simulate over 70,000 days of the atmosphere in the time it would take a physics-based model to simulate only 19 days. And\nGraphCast\nwon the\n2024 MacRobert Award\nfor engineering innovation.\nThis selection of GraphCast\u2019s predictions rolling across 10 days shows specific humidity at 700 hectopascals (about 3 kilometers above surface), surface temperature and surface wind speed.\nWe also improved our\nflood forecasting model\nto predict flooding seven days in advance (up from five) and expanded our riverine flood forecasting coverage to 100 countries and 700 million people. This marks a significant milestone in a multi-year initiative that Google Research embarked on in 2018.\nOur flood forecasting model is now available in over 100 countries (left), and we now have \u201cvirtual gauges\u201d for experts and researchers in more than 150 countries, including countries where physical gauges are not available.\nAI can also help with wildfire detection and mitigation, which is especially top of mind given the devastation in California. Our\nWildfire Boundary Maps capabilities\nare now available in 22 countries. Alongside leading wildfire authorities, Google Research also created\nFireSat\n, a constellation of satellites that can detect and track wildfires as small as a classroom (roughly 5x5 meters) within 20 minutes.\nAnd we continued building on our commitment to making more information more accessible to more people,\nexpanding Google Translate\nwith 110 new languages, including Cantonese, Papua New Guinea\u2019s Tok Pisin, N\u2019Ko from West Africa and Manx from the Isle of Man. Google Translate \u2014 which now supports over 240 languages \u2014 can help people overcome barriers to information, knowledge and opportunity.\nThese new languages in Google Translate represent more than 614 million speakers, opening up translations for around 8% of the world\u2019s population.\nHelping set the standard in responsible AI\nWe furthered our industry-leading research in AI safety, developing new tools and techniques and integrating these advances into our latest models. We\u2019re committed to working with others to address risks.\nWe continued\nresearching\nmisuse, conducting a study that found the two most common types of misuse were deep fakes and jailbreaks. In May, we introduced\nThe Frontier Safety Framework\n, which established protocols for identifying the emerging capabilities of our most advanced AI models, and launched our\nAI Responsibility Lifecycle framework\nto the public. In October, we\nexpanded\nour\nResponsible GenAI Toolkit\nto work with any LLM, giving developers more tools to build AI responsibly.\nAnd, among our other efforts, we released a paper this year on\nThe Ethics of Advanced AI Assistants\nthat examined and mapped the new technical and moral landscape of a future populated by AI assistants, and characterized the opportunities and risks society might face.\nWe expanded\nSynthID\u2019s capabilities\nto watermarking AI-generated text in the\nGemini app and web experience\n, and video in\nVeo\n. To help increase overall transparency online, not just with content created by Google gen AI tools, we also\njoined\nthe Coalition for Content Provenance and Authenticity (C2PA) as a steering committee member and\ncollaborated\non a new, more secure version of the technical standard, Content Credentials.\nWhen there\u2019s a range of different tokens to choose from, SynthID can adjust the probability score of each predicted token, in cases where it won\u2019t compromise the quality, accuracy and creativity of the output.\nBeyond LLMs, we shared our approach to\nbiosecurity\nfor\nAlphaFold 3\n. We also worked with industry partners to launch the\nCoalition for Secure AI\n(CoSAI), and we participated in the\nAI Seoul Summit\n, as a way of building and contributing to an international consensus and a common, coordinated approach to governance.\nAs we develop new technologies like AI agents, we\u2019ll continue to encounter new questions around safety, security and privacy. Guided by our\nAI Principles\n, we are\ndeliberately taking\nan exploratory and gradual approach to development, conducting research on multiple prototypes, iteratively implementing safety training, working with trusted testers and external experts and performing extensive risk assessments and safety and assurance evaluations.\nLooking ahead to 2025\n2024 was a productive year, and a very exciting time for groundbreaking new products and research in AI. We made a great deal of progress and we\u2019re even more excited about the year ahead.\nAs we continue to produce groundbreaking AI research in the fields of products, science, health, creativity and more, it becomes increasingly important to think deeply about how and when it should be deployed. By continuing to prioritize responsible AI practices and fostering collaboration, we\u2019ll play an important role in building a future where AI benefits humanity.\nPOSTED IN:\nRelated stories\nGoogle Workspace\nHow AI made Meet\u2019s language translation possible\nBy\nMolly McHugh-Johnson\nSep 11, 2025\nAI\nThe latest AI news we announced in August\nBy\nKeyword Team\nSep 10, 2025\nLearning & Education\nAI Quests: Bringing AI literacy to the classroom\nBy\nRonit Levavi Morad\nSep 09, 2025\nAI\nThe latest Google AI literacy resources all in one place\nBy\nJennie Magiera\nResearch\nGoogle Quantum AI has been selected for the DARPA Quantum Benchmarking Initiative.\nBy\nHartmut Neven\nSep 09, 2025\nSearch\nGoogle Doodles show how AI Mode can help you learn.\nSep 08, 2025\n.\nJump to position 1\nJump to position 2\nJump to position 3\nJump to position 4\nJump to position 5\nJump to position 6\nLet\u2019s stay in touch. Get the latest news from Google in your inbox.\nSubscribe\nNo thanks",
        "image_urls": [
          {
            "url": "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Demis_headshot.max-244x184.format-webp.webp",
            "score": 0
          },
          {
            "url": "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/2024_Headshot_for_James_Manyika.max-244x184.format-webp.webp",
            "score": 0
          },
          {
            "url": "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Jeff_Dean_Photo_1.max-244x184.format-webp.webp",
            "score": 0
          },
          {
            "url": "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/FACTS_system_instruction2x.width-100.format-webp.webp",
            "score": 0
          },
          {
            "url": "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/EOY-2024-Figure-250114-r01.width-100.format-webp.webp",
            "score": 0
          },
          {
            "url": "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Fig_1.width-100.format-webp.webp",
            "score": 0
          },
          {
            "url": "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Social_03.width-100.format-webp_GDqJDqv.webp",
            "score": 0
          },
          {
            "url": "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/AF_hero_2_crop.width-100.format-webp.webp",
            "score": 0
          },
          {
            "url": "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Med-Gemini-2b-MedQA.width-100.format-webp.webp",
            "score": 0
          },
          {
            "url": "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Flood-Forecasting.width-100.format-webp.webp",
            "score": 0
          }
        ],
        "title": "Year in review: Google's biggest AI advancements of 2024"
      },
      {
        "url": "https://blog.google/technology/ai/google-ai-big-scientific-breakthroughs-2024/",
        "raw_content": "How Google AI is advancing science\n9 ways AI is advancing science\nNov 18, 2024\n\u00b7\nShare\nTwitter\nFacebook\nLinkedIn\nMail\nCopy link\nWe\u2019re sharing a recap of some of the biggest scientific breakthroughs in recent years brought about by AI.\nKeyword Team\nRead AI-generated summary\nGeneral summary\nAI is rapidly advancing science, with breakthroughs in fields like protein structure prediction, brain mapping, and flood forecasting. These advancements are built on collaborations between researchers, technologists, and policymakers, and they offer a blueprint for how AI can improve human life. The Royal Society and Google DeepMind are hosting the first AI for Science Forum to discuss the transformative potential of AI and the role of public-private partnerships in innovation.\nSummaries were generated by Google AI. Generative AI is experimental.\nBullet points\nAI is rapidly advancing science, leading to breakthroughs in fields like healthcare, energy, and materials science.\nAI models like AlphaFold are predicting protein structures, accelerating drug development and tackling environmental issues.\nAI is helping us understand the human brain in unprecedented detail, aiding in health research and treatment development.\nAI is improving weather forecasting, enabling more accurate predictions and better preparedness for extreme events.\nAI is being used to develop new materials, potentially leading to more efficient solar cells, batteries, and superconductors.\nSummaries were generated by Google AI. Generative AI is experimental.\nExplore other styles:\nGeneral summary\nBullet points\nShare\nTwitter\nFacebook\nLinkedIn\nMail\nCopy link\nLast updated: November 22, 2024\nWe\u2019re living in a time when applied science, human ingenuity and new technologies are offering deep insights into some of humanity\u2019s biggest (and oldest) questions. While we often think of scientific progress as fast and unrelenting, for many decades, progress has\nactually slowed\n. While the scientific community continues to debate the cause of this slowdown, much of today's technology \u2014 from jets to manufacturing processes \u2014 is not significantly different than half a century ago.\nBut in just the past few years, breakthroughs in formerly nascent fields like artificial intelligence and quantum computing have dramatically accelerated the pace of scientific discovery. And from healthcare advances to finding plastic-eating enzymes, we\u2019re already benefiting from it.\nThese breakthroughs are built on decades of collaboration between researchers, technologists, policymakers, civil organizations and many people from across society. And they offer a blueprint for how applying AI to science can dramatically improve human life.\nIt\u2019s with this in mind that today The Royal Society in partnership with Google DeepMind is cohosting the first AI for Science Forum. This event in London brings together the scientific community, policymakers, and industry leaders to look at the transformative potential of AI to accelerate science and the role of public-private partnerships in innovation.\nTo explore how we got here and where we can go next, here\u2019s a look at nine recent moments that have set the stage for so much of the scientific progress on the horizon:\n1. Cracking the 50-year \u201cgrand challenge\u201d of protein structure prediction\nExperts have described demystifying protein folding as a \"grand challenge\" for decades. In 2022, Google DeepMind shared the predicted structures of 200 million proteins from their\nAlphaFold 2 model\n. Previously, determining the 3D structure of a single protein typically took a year or more \u2014 AlphaFold can predict these shapes with remarkable accuracy in minutes. By releasing the protein structure predictions in\na free database\n, this has enabled scientists around the world to accelerate progress in areas like developing\nnew medicines\n,\nfighting antibiotic resistance\nand\ntackling plastic pollution\n. As a next step,\nthe AlphaFold 3 model builds on AlphaFold 2 to predict the structure and interaction\nof all of life\u2019s molecules.\n2. Showing the human brain in unprecedented detail, to support health research\nFew things have held more mystery throughout time than the human brain. Developed over 10 years of\nconnectomics\nresearch,\nGoogle partnered with others, including the the Lichtman Lab at Harvard\n, to map a tiny piece of the human brain to a level of detail never previously achieved. This project, released in 2024, revealed never-before-seen structures within the human brain. And the full dataset, including AI-generated annotations for each cell, has been made publicly available to help accelerate research.\n3. Saving lives with accurate flood forecasting\nWhen Google\u2019s flood forecasting project\nbegan in 2018\n, many believed it was impossible to accurately deliver flood forecasting at scale, given the scarcity of data. But researchers were able to develop an AI model that achieves reliability in predicting extreme riverine events in ungauged watersheds at up to a five-day lead time with reliability matching or exceeding that of nowcasts (zero-day lead time). In 2024, Google Research expanded this coverage to\n100 countries and 700 million people worldwide\n\u2014 and improved the AI model so it offers the same accuracy at a seven-day lead time as the previous model had at five.\n4. Spotting wildfires earlier to help firefighters stop them faster\nWildfires are increasingly upending communities around the world due to hotter and drier climates. In 2024,\nGoogle Research partnered with the U.S. Forest Service to develop FireSat\n, an AI model and new global satellite constellation designed specifically to detect and track wildfires the size of a classroom by providing higher-resolution imagery within 20 minutes. This will allow fire authorities to respond more quickly, potentially saving lives, property and natural resources.\n5. Predicting weather faster and with more accuracy\nIn 2023, Google DeepMind launched and open sourced the model code for\nGraphCast\n, a machine learning research model that predicts weather conditions up to 10 days in advance more accurately and much faster than the industry gold-standard weather simulation system (HRES). GraphCast can also predict the tracks of cyclones (and associated risks like flooding) with greater accuracy,\nand accurately predicted Hurricane Lee\nwould hit Nova Scotia three days before traditional models.\n6. Advancing the frontier of mathematical reasoning\nAI has always struggled with complex math due to a lack of data and reasoning skills. Then, in 2024, Google DeepMind announced\nAlphaGeometry\n, an AI system that solved complex geometry problems at a level approaching a human Olympiad gold-medalist \u2014 a breakthrough in AI performance and the pursuit of more advanced general AI systems. The subsequent Gemini-trained model,\nAlphaGeometry 2, was then combined with a new model AlphaProof\n, and together they solved 83% of all historical International Mathematical Olympiad (IMO) geometry problems from the past 25 years. In demonstrating AI\u2019s growing ability to reason, and potentially solve problems beyond current human abilities, this moved us closer to systems that can discover and verify new knowledge.\n7. Using quantum computing to accurately predict chemical reactivity and kinetics\nGoogle researchers worked with UC Berkeley and Columbia University to perform the largest chemistry simulations to date on a quantum computer. The results,\npublished in 2022\n, were not only competitive with classical methods, but they also did not require the burdensome error mitigation typically associated with quantum computing. The ability to conduct these simulations will offer even more accurate predictions of chemical reactivity and kinetics, which is a precursor for applying chemistry in new ways to help solve real-world challenges.\n8. Accelerating materials science and the potential for more sustainable solar cells, batteries and superconductors\nIn 2023, Google DeepMind announced Graph Networks for Materials Exploration (\nGNoME)\n, a new AI tool that has already discovered 380,000 materials that are stable at low temperatures, according to simulations. At a time when our world is looking for new approaches to energy, processing power and materials science, this work could pave the way to\nbetter solar cells, batteries\nand potential superconductors. Plus, to help this technology benefit everyone, Google DeepMind made GNoME\u2019s most stable predictions available via the Materials Project on their open database.\n9. Taking a meaningful step toward nuclear fusion \u2014 and abundant clean energy\nAs the old joke goes, \u201cFusion is the energy of the future \u2014 and it always will be.\u201d Controlling and using the energy that fuels stars (including our own sun) has been beyond the realm of science. In 2022,\nGoogle DeepMind announced\nthat it developed AI that can\ncontrol the plasma inside a nuclear fusion reactor\nautonomously. By collaborating with the Swiss Plasma Center at EPFL, Google DeepMind built the first Reinforcement Learned system capable of autonomously stabilizing and shaping the plasma within an operational fusion reactor, opening up new pathways toward stable fusion and abundant clean energy for everyone.\nPOSTED IN:\nRelated stories\nGoogle Workspace\nHow AI made Meet\u2019s language translation possible\nBy\nMolly McHugh-Johnson\nSep 11, 2025\nAI\nThe latest AI news we announced in August\nBy\nKeyword Team\nSep 10, 2025\nSustainability\nGoogle is fighting water leaks in Belgium.\nBy\nBen Townsend\nSep 10, 2025\nLearning & Education\nAI Quests: Bringing AI literacy to the classroom\nBy\nRonit Levavi Morad\nSep 09, 2025\nAI\nThe latest Google AI literacy resources all in one place\nBy\nJennie Magiera\nResearch\nGoogle Quantum AI has been selected for the DARPA Quantum Benchmarking Initiative.\nBy\nHartmut Neven\nSep 09, 2025\n.\nJump to position 1\nJump to position 2\nJump to position 3\nJump to position 4\nJump to position 5\nJump to position 6\nLet\u2019s stay in touch. Get the latest news from Google in your inbox.\nSubscribe\nNo thanks",
        "image_urls": [
          {
            "url": "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/superG_v3.max-244x184.format-webp.webp",
            "score": 0
          },
          {
            "url": "https://storage.googleapis.com/gweb-uniblog-publish-prod/original_images/Meet_Speech_Translate_with_AI.png",
            "score": 0
          },
          {
            "url": "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/August_latest_AI_News.width-300.format-webp.webp",
            "score": 0
          },
          {
            "url": "https://storage.googleapis.com/gweb-uniblog-publish-prod/original_images/image_7_dHwLXVL.png",
            "score": 0
          },
          {
            "url": "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/03-ai-literacy-social-sharing-192.width-300.format-webp.webp",
            "score": 0
          },
          {
            "url": "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/122-B2S-EDU-AI-Literacy-Blog-Head.width-300.format-webp.webp",
            "score": 0
          },
          {
            "url": "https://storage.googleapis.com/gweb-uniblog-publish-prod/images/QuantumChip_Social.width-300.format-webp.webp",
            "score": 0
          },
          {
            "url": "https://blog.google/static/blogv2/images/newsletter_toast.svg?version=pr20250903-2044",
            "score": 0
          }
        ],
        "title": "How Google AI is advancing science"
      },
      {
        "url": "https://www.forbes.com/sites/bernardmarr/2024/12/16/6-game-changing-ai-breakthroughs-that-defined-2024/",
        "raw_content": "6 Game-Changing AI Breakthroughs That Defined 2024\nInnovation\nEnterprise Tech\n6 Game-Changing AI Breakthroughs That Defined 2024\nBy\nBernard Marr\n,\nContributor.\nFollow Author\nDec 16, 2024, 02:09am EST\nShare\nSave\nComment\nFrom Apple\u2019s entry into generative AI to unprecedented achievements in robotics and art, 2024 marked a transformative year in artificial intelligence innovation.\nAdobe Stock\nWithout a doubt, 2024 will go down as a year in which we saw much groundbreaking progress and many giant leaps forward in the development of artificial intelligence.\nWith companies racing to build AI features and functionalities into products, we\u2019re increasingly getting used to automation in the workplace, and AI is becoming integrated across all aspects of our everyday lives.\nSo, with the AI revolution well underway, here\u2019s my overview of the year\u2019s AI highlights \u2013 including the game-changing breakthroughs and discoveries setting the stage for an even more momentous 2025.\nThe Continuing Evolution And Advancement Of Generative AI Chatbots\nWhen ChatGPT emerged two years ago (was that all it was?), it was clear that what we were seeing was just the beginning. Since then, we\u2019ve seen rival chatbots emerging from established tech leaders, including Google and Meta, fellow AI startups such as Anthropic, and many\nopen-source\ncollaborations and projects. New features like memory and multimodal capabilities have broken new ground in terms of what we expect from AI. And the most exciting thing is we\u2019re still clearly only just getting started.\nApple Enters The Generative AI Arena With Apple Intelligence\nIf there\u2019s one thing that Apple does really well, it\u2019s taking a great idea and launching it into the mainstream. This is why the\n2024 arrival of Apple Intelligence\ncould well go down as a watershed moment in consumer adoption of AI. By integrating OpenAI-powered generative language and graphics functionality across its product ecosystem, it created a typically refined, Apple-shaped gateway into the world of day-to-day AI for millions of non-techy people.\nMORE FROM\nFORBES ADVISOR\nBest High-Yield Savings Accounts Of 2024\nBy\nKevin Payne\nContributor\nBest 5% Interest Savings Accounts of 2024\nBy\nCassidy Horton\nContributor\nHead Of Google AI Wins The Nobel Prize For Chemistry\nDemis Hassabis has said that the most important effect of the AI revolution is that it will act as an accelerator in many other fields of science. In 2024, he demonstrated this when he was made\njoint winner\nof the Nobel Prize for chemistry, thanks to the AI model AlphaFold 2 being integral to the task of creating new proteins. By predicting the complex amino acid sequences needed, it is thought that this breakthrough will lead to new developments in medicines, vaccines and material sciences.\nThe EU AI Act \u2013 AI Legislation Begins To Take Shape\nIn August 2024, the EU\u2019s Artificial Intelligence Act\ncame into force\n, marking a significant step by an international regulator to implement a framework and safeguards around AI. Rather than conferring rights on individuals (as is the case with the union\u2019s data protection regulation, GDPR), it aims to impose controls on providers of AI services. This is done by categorizing applications according to their potential for causing harm, regulating some while outright banning those classed as \u201cunacceptable.\u201d This highest-risk category includes applications that enable facial recognition technology to be used in public places or for social scoring.\nOptimus Breaks New Ground For Humanoid Robots\nTesla\ndemonstrated\nthe latest iteration of its humanoid robot, codenamed Optimus after the Transformers character, in front of a stunned audience at its We Robot event in October. Despite controversy over how much of its operation was automated and how much was simply remote-controlled via telepresence, experts agreed that it showed impressive progress toward the development of bipedal robots that could eventually assist with many tasks in homes and industry.\nPainting Created By AI Robot Sells For $1 Million\nThe last couple of years have seen an explosion in AI art \u2013 but perhaps the most mind-blowing milestone was passed when Ai-Da became the first humanoid robot to sell a piece of artwork at auction.\nThe painting, titled AI God, was estimated to sell for between $120,000 and $180,000, but\nbidding\nsoared to a final $1,084,000 at the historic Sotheby's auction house in London in February this year.\nAs we reflect on 2024's remarkable AI achievements, it's clear that we're witnessing not just technological advancement but a fundamental transformation in how AI integrates into our lives and work. As we look toward 2025, these developments suggest we're entering an era where AI's impact will continue to expand and evolve in ways that promise to reshape every aspect of human endeavor.\nEditorial Standards\nReprints & Permissions",
        "image_urls": [
          {
            "url": "https://specials-images.forbesimg.com/imageserve/675fd1ce122216c83e2c53fc/From-Apple-s-entry-into-generative-AI-to-unprecedented-achievements-in-robotics-and/960x0.jpg?fit=scale",
            "score": 0
          },
          {
            "url": "https://thumbor.forbes.com/thumbor/fit-in/1290x/https://www.forbes.com/advisor/wp-content/uploads/2020/12/getty_1-best-online-savings-thumbnail_101920pm.jpg",
            "score": 0
          },
          {
            "url": "https://thumbor.forbes.com/thumbor/fit-in/900x510/https://www.forbes.com/advisor/wp-content/uploads/2023/09/Saving-Rates-2.jpg",
            "score": 0
          }
        ],
        "title": "6 Game-Changing AI Breakthroughs That Defined 2024"
      }
    ]
  },
  "tavily_Climate Research": {
    "success": true,
    "urls_found": 3,
    "urls_scraped": 2,
    "retrieval_time": 1.3568589687347412,
    "scraping_time": 0.9035391807556152,
    "total_time": 2.2603981494903564,
    "content_analysis": {
      "successful_scrapes": 2,
      "failed_scrapes": 0,
      "total_content_length": 7585,
      "total_images": 10,
      "relevance_score": 50.0,
      "content_samples": [
        {
          "url": "https://www.nature.com/subjects/climate-change",
          "title": "Climate change - Latest research and news | Nature",
          "content_length": 7356,
          "keyword_matches": 5,
          "sample_content": "Climate change - Latest research and news | Nature\nSkip to main content\nThank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain the best experience, ..."
        },
        {
          "url": "https://www.sciencedirect.com/science/article/pii/S2590332225001113",
          "title": "ScienceDirect",
          "content_length": 229,
          "keyword_matches": 0,
          "sample_content": "ScienceDirect\nSkip to main content\nAre you a robot?\nPlease confirm you are a human by completing the captcha challenge below.\nEnable JavaScript and cookies to continue\nReference number:\n97db46df8c6f42..."
        }
      ],
      "titles": [
        "Climate change - Latest research and news | Nature",
        "ScienceDirect"
      ]
    },
    "search_results": [
      {
        "href": "https://www.un.org/en/climatechange/reports",
        "body": "Climate change is widespread, rapid and intensifying. That is the key finding of the latest scientific report from the Intergovernmental Panel on Climate Change"
      },
      {
        "href": "https://www.nature.com/subjects/climate-change",
        "body": "Latest Research and Reviews. Neglecting land\u2013atmosphere feedbacks overestimates climate-driven increases in evapotranspiration."
      },
      {
        "href": "https://www.sciencedirect.com/science/article/pii/S2590332225001113",
        "body": "by R Schaeffer \u00b7 2025 \u00b7 Cited by 6 \u2014 10 key advances in climate-change research with high policy relevance. The insights span a wide range of areas, from changes in methane and aerosol emissions."
      }
    ],
    "scraped_results": [
      {
        "url": "https://www.nature.com/subjects/climate-change",
        "raw_content": "Climate change - Latest research and news | Nature\nSkip to main content\nThank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain the best experience, we recommend you use a more up to date browser (or turn off compatibility mode in Internet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles and JavaScript.\nAdvertisement\nClimate change articles from across Nature Portfolio\nAtom\nRSS Feed\nDefinition\nClimate change refers to a statistically defined change in the average and/or variability of the climate system, this includes the atmosphere, the water cycle, the land surface, ice and the living components of Earth. The definition does not usually require the causes of change to be attributed, for example to human activity, but there are exceptions.\nFeatured\nHeatwaves linked to emissions of individual fossil-fuel and cement producers\nThe emissions of leading fossil-fuel and cement producers have been systematically linked to particular heatwaves. Three scientists discuss the methodology behind the result and its potential impact on climate-liability court cases.\nKarsten Haustein\nMichael B. Gerrard\nJessica A. Wentz\nNews & Views\n10 Sept 2025\nNature\nVolume: 645, P: 319-320\nClosing emission gaps in border carbon adjustments for chemicals and plastics\nMajor greenhouse gas emissions from chemicals and plastics are overlooked under the current design of the European Union\u2019s Carbon Border Adjustment Mechanism. To close these important gaps in coverage, policymakers should include fossil-based feedstocks and raise country-specific default emissions values to ensure fair and comprehensive carbon accounting.\nHannah Minten\nJulian Hausweiler\nAndr\u00e9 Bardow\nNews & Views\n10 Sept 2025\nNature Sustainability\nP: 1-2\nPainting humid cities cool\nPassive radiative paints cool buildings without energy input, but do not perform well in humid environments and on vertical surfaces. Now, researchers report a durable cement-based paint that integrates radiative cooling and evaporative cooling mechanisms, achieving effective cooling on vertical surfaces in humid climates while maintaining the mechanical strength and substrate adhesion required for real-world building applications.\nYing Liu\nDangyuan Lei\nNews & Views\n09 Sept 2025\nNature Energy\nP: 1-2\nRelated Subjects\nAttribution\nClimate and Earth system modelling\nClimate-change impacts\nClimate-change mitigation\nProjection and prediction\nLatest Research and Reviews\nNeglecting land\u2013atmosphere feedbacks overestimates climate-driven increases in evapotranspiration\nHow evapotranspiration changes with warming is not well understood. Here the authors show that when often-neglected land\u2013atmosphere feedbacks are considered, evapotranspiration increases less than currently projected by offline models.\nSha Zhou\nBofu Yu\nResearch\n11 Sept 2025\nNature Climate Change\nP: 1-8\nSystematic attribution of heatwaves to the emissions of carbon majors\nClimate change made 213 historical heatwaves reported over 2000\u20132023 more likely and more intense, to which each of the 180 carbon majors (fossil fuel and cement producers) substantially contributed.\nYann Quilcaille\nLukas Gudmundsson\nSonia I. Seneviratne\nResearch\nOpen Access\n10 Sept 2025\nNature\nVolume: 645, P: 392-398\nCausal inference unveils how forest coverage mitigates excess snakebite cases during rainfall seasons in Colombia\nJuan David Guti\u00e9rrez\nCarlos Bravo-Vega\nJuan Manuel Cordovez\nResearch\nOpen Access\n10 Sept 2025\nScientific Reports\nVolume: 15, P: 32401\nHysteresis and reversibility of agroecological droughts in response to carbon dioxide removal\nUsing an idealized multi-model experiment and a new hysteresis quantification method, this study shows that equivalent carbon dioxide removal fails to symmetrically reverse CO\n2\n-emissions-induced agroecological droughts, revealing irreversible impacts in hotspots in the Mediterranean, northern Central America, southern Africa and southern Australia, necessitating urgent adaptation planning.\nLaibao Liu\nMathias Hauser\nSonia I. Seneviratne\nResearch\nOpen Access\n10 Sept 2025\nNature Water\nP: 1-8\nPleistocene terrestrial warming trend in East Asia linked to Antarctic ice sheets growth\nThe authors quantify terrestrial temperature evolution over the past 2 million years by fossil lipids preserved in an ancient lake in East Asia. They showed a long-term warming trend that diverges from the contemporaneous global sea surface cooling.\nHuanye Wang\nWeiguo Liu\nZhisheng An\nResearch\nOpen Access\n10 Sept 2025\nNature Communications\nVolume: 16, P: 8258\nEmbodied emissions of chemicals within the EU Carbon Border Adjustment Mechanism\nThe effects of including the chemical industry in the existing Carbon Border Adjustment Mechanism of the European Union are unclear. A study finds that the current framework covers only half of key chemical emissions, urging the addition of fossil feedstocks and tougher default rules to boost efficacy.\nHannah Minten\nJulian Hausweiler\nAndr\u00e9 Bardow\nResearch\nOpen Access\n10 Sept 2025\nNature Sustainability\nP: 1-10\nAll Research & Reviews\nNews and Comment\nTrump team disbands controversial US climate panel\nA report by the panel downplays the ills of global warming and was key to White House efforts to revoke federal authority to regulate climate.\nJeff Tollefson\nNews\n11 Sept 2025\nNature\nFeeling the heat: fossil-fuel producers linked to dozens of heatwaves\nAttribution study suggests major energy producers play an outsized role in causing extreme heatwaves \u2014 plus, the scientists fighting back against US funding cuts.\nBenjamin Thompson\nShamini Bundell\nNews\n10 Sept 2025\nNature\nClimate impacts are real \u2014 denying this is self-defeating\nThe US administration is attempting to undermine efforts to curb greenhouse-gas emissions. It will ultimately leave that country, and the world, worse off.\nEditorial\n10 Sept 2025\nNature\nVolume: 645, P: 284\nHeatwaves linked to emissions of individual fossil-fuel and cement producers\nThe emissions of leading fossil-fuel and cement producers have been systematically linked to particular heatwaves. Three scientists discuss the methodology behind the result and its potential impact on climate-liability court cases.\nKarsten Haustein\nMichael B. Gerrard\nJessica A. Wentz\nNews & Views\n10 Sept 2025\nNature\nVolume: 645, P: 319-320\nClosing emission gaps in border carbon adjustments for chemicals and plastics\nMajor greenhouse gas emissions from chemicals and plastics are overlooked under the current design of the European Union\u2019s Carbon Border Adjustment Mechanism. To close these important gaps in coverage, policymakers should include fossil-based feedstocks and raise country-specific default emissions values to ensure fair and comprehensive carbon accounting.\nHannah Minten\nJulian Hausweiler\nAndr\u00e9 Bardow\nNews & Views\n10 Sept 2025\nNature Sustainability\nP: 1-2\nHeatwaves linked to carbon emissions from specific companies\nNearly one-quarter of heatwaves would have been \u2018virtually impossible\u2019 without global warming \u2014 and can be attributed to the emissions of individual energy producers.\nJeff Tollefson\nNews\n10 Sept 2025\nNature\nAll News & Comment\nSearch\nSearch articles by subject, keyword or author\nShow results from\nAll journals\nSearch\nAdvanced search\nQuick links\nExplore articles by subject\nFind a job\nGuide to authors\nEditorial policies",
        "image_urls": [
          {
            "url": "https://pubads.g.doubleclick.net/gampad/ad?iu=/285/nature.com/about&sz=728x90&pos=top;type=about;path=/subjects/climate-change",
            "score": 0
          },
          {
            "url": "https://media.springernature.com/w290h158/magazine-assets/d41586-025-02596-6/d41586-025-02596-6_51433842.jpg",
            "score": 0
          },
          {
            "url": "https://media.springernature.com/w290h158/springer-static/image/art%3A10.1038%2Fs41893-025-01622-9/MediaObjects/41893_2025_1622_Fig1_HTML.png",
            "score": 0
          },
          {
            "url": "https://media.springernature.com/w290h158/springer-static/image/art%3A10.1038%2Fs41560-025-01858-x/MediaObjects/41560_2025_1858_Fig1_HTML.png",
            "score": 0
          },
          {
            "url": "https://media.springernature.com/w290h158/springer-static/image/art%3A10.1038%2Fs41558-025-02428-5/MediaObjects/41558_2025_2428_Fig1_HTML.png",
            "score": 0
          },
          {
            "url": "https://media.springernature.com/w290h158/springer-static/image/art%3A10.1038%2Fs41586-025-09450-9/MediaObjects/41586_2025_9450_Fig1_HTML.png",
            "score": 0
          },
          {
            "url": "https://media.springernature.com/w290h158/springer-static/image/art%3A10.1038%2Fs41598-025-17405-3/MediaObjects/41598_2025_17405_Fig1_HTML.png",
            "score": 0
          },
          {
            "url": "https://media.springernature.com/w290h158/springer-static/image/art%3A10.1038%2Fs44221-025-00487-8/MediaObjects/44221_2025_487_Fig1_HTML.png",
            "score": 0
          },
          {
            "url": "https://media.springernature.com/w290h158/springer-static/image/art%3A10.1038%2Fs41467-025-63331-3/MediaObjects/41467_2025_63331_Fig1_HTML.png",
            "score": 0
          },
          {
            "url": "https://media.springernature.com/w290h158/springer-static/image/art%3A10.1038%2Fs41893-025-01618-5/MediaObjects/41893_2025_1618_Fig1_HTML.png",
            "score": 0
          }
        ],
        "title": "Climate change - Latest research and news | Nature"
      },
      {
        "url": "https://www.sciencedirect.com/science/article/pii/S2590332225001113",
        "raw_content": "ScienceDirect\nSkip to main content\nAre you a robot?\nPlease confirm you are a human by completing the captcha challenge below.\nEnable JavaScript and cookies to continue\nReference number:\n97db46df8c6f42d7\nIP Address:\n199.79.156.163",
        "image_urls": [],
        "title": "ScienceDirect"
      }
    ]
  },
  "tavily_Programming Tutorials": {
    "success": true,
    "urls_found": 3,
    "urls_scraped": 2,
    "retrieval_time": 1.6964972019195557,
    "scraping_time": 0.7718708515167236,
    "total_time": 2.4683680534362793,
    "content_analysis": {
      "successful_scrapes": 2,
      "failed_scrapes": 0,
      "total_content_length": 50551,
      "total_images": 10,
      "relevance_score": 100.0,
      "content_samples": [
        {
          "url": "https://realpython.com/python-web-scraping-practical-introduction/",
          "title": "A Practical Introduction to Web Scraping in Python \u2013 Real Python",
          "content_length": 50310,
          "keyword_matches": 5,
          "sample_content": "A Practical Introduction to Web Scraping in Python \u2013 Real Python\n\u2014 FREE Email Series \u2014\n\ud83d\udc0d Python Tricks \ud83d\udc8c\nGet Python Tricks \u00bb\n\ud83d\udd12 No spam. Unsubscribe any time.\nBrowse Topics\nGuided Learning Paths\nBasics..."
        },
        {
          "url": "https://www.youtube.com/watch?v=DcI_AZqfZVc",
          "title": "Advanced Web Scraping Tutorial! (w/ Python Beautiful Soup Library) - YouTube",
          "content_length": 241,
          "keyword_matches": 4,
          "sample_content": "Advanced Web Scraping Tutorial! (w/ Python Beautiful Soup Library) - YouTube\nAbout\nPress\nCopyright\nContact us\nCreators\nAdvertise\nDevelopers\nTerms\nPrivacy\nPolicy & Safety\nHow YouTube works\nTest new fea..."
        }
      ],
      "titles": [
        "A Practical Introduction to Web Scraping in Python \u2013 Real Python",
        "Advanced Web Scraping Tutorial! (w/ Python Beautiful Soup Library) - YouTube"
      ]
    },
    "search_results": [
      {
        "href": "https://realpython.com/python-web-scraping-practical-introduction/",
        "body": "This tutorial guides you through extracting data from websites using string methods, regular expressions, and HTML parsers."
      },
      {
        "href": "https://www.youtube.com/watch?v=DcI_AZqfZVc",
        "body": "Advanced Web Scraping Tutorial! (w/ Python Beautiful Soup Library)\nKeith Galli\n249000 subscribers\n1491 likes\n50938 views\n8 Jun 2024\nGet started w/ Bright Data + $15 free credit using this link!\nhttps://brdta.com/keithgalli\n\nIn this video, we're diving into advanced web scraping techniques with Python. If you haven't seen my overview of the Beautiful Soup library, check it out first for some foundational knowledge. Web scraping is a highly valuable skill, especially for freelance work. This tutorial will take you through sophisticated scraping methods, using Walmart as an example.\n\nBefore we start, a big thank you to our sponsor, Bright Data. They offer proxy tools that make advanced web scraping much easier, allowing you to bypass restrictions set by websites. Check out their data sets marketplace for quick access to various data.\n\nIn this video, we'll cover:\n- Setting up and understanding the HTML structure of a web page\n- Extracting data using Beautiful Soup and handling dynamic content\n- Implementing headers to avoid detection\n- Parsing JSON data for efficient scraping\n- Using proxies with Bright Data to bypass IP blocking\n- Error handling and retries in scraping\n- Storing scraped data and handling multiple search queries\n\nIf you need help getting started with web scraping, check out my original tutorial on BeautifulSoup:\nhttps://youtu.be/GjKQ6V_ViQE?si=f9Xo0ING4fNLhLx2\n\nHelpful Links:\nGitHub Repository with Code Examples: https://github.com/KeithGalli/advanced-scraping\n\nVideo Timeline!\n0:00 - Intro & Overview\n1:30 - Identifying HTML Structure for Scraping (from Walmart)\n4:26 - Writing Python BeautifulSoup Code to Extract Info from Walmart.com\n7:22 - Implementing modified request headers to avoid detection\n6:10 - Handling Dynamic Content\n8:00 - Implementing Modified Request Headers to Avoid Detection (look more human when scraping)\n9:30 - Parsing Complicated JSON Data (Using LLMs to help)\n15:28 - Extending our Code to Collect Info on Many Products (Automating Search)\n24:45 - Improving our Code (avoiding duplicates, multiple search terms, using a queue, etc.)\n27:20 - Setting Up Proxies with Bright Data (Get around IP Address blocks)\n36:35 - Error Handling and Retries\n39:36  - Automating actions on pages with Selenium \n41:42 - Conclusion & Next Steps\n\nI hope you find this tutorial useful. If you did, please give it a thumbs up and subscribe to the channel for more tutorials. Let me know in the comments how you plan to use these web scraping techniques in your projects. Enjoy scraping!\n\n-------------------------\nFollow me on social media!\nInstagram | https://www.instagram.com/keithgalli/\nTwitter | https://twitter.com/keithgalli\nTikTok | https://tiktok.com/@keithgalli\n\n-------------------------\nPractice your Python Pandas data science skills with problems on StrataScratch!\nhttps://stratascratch.com/?via=keith\n\nJoin the Python Army to get access to perks!\nYouTube - https://www.youtube.com/channel/UCq6XkhO5SZ66N04IcPbqNcw/join\nPatreon - https://www.patreon.com/keithgalli\n\n*I use affiliate links on the products that I recommend. I may earn a purchase commission or a referral bonus from the usage of these links.\n74 comments\n"
      },
      {
        "href": "https://www.geeksforgeeks.org/python-web-scraping-tutorial/",
        "body": "In this tutorial, you'll learn how to use these Python tools to scrape data from websites and understand why Python 3 is a popular choice for web scraping tasks. In this example, we'll extract all paragraph (<p>) text from the main content section of the GeeksforGeeks Python Tutorial page. To handle this, we use Selenium that can automate browsers like Chrome or Firefox, wait for content to load, click buttons, scroll and extract fully rendered web pages just like a real user. The urllib module in Python is a built-in library that provides functions for working with URLs. It allows you to interact with web pages by fetching URLs (Uniform Resource Locators), opening and reading data from them and performing other URL-related tasks like encoding and parsing."
      }
    ],
    "scraped_results": [
      {
        "url": "https://realpython.com/python-web-scraping-practical-introduction/",
        "raw_content": "A Practical Introduction to Web Scraping in Python \u2013 Real Python\n\u2014 FREE Email Series \u2014\n\ud83d\udc0d Python Tricks \ud83d\udc8c\nGet Python Tricks \u00bb\n\ud83d\udd12 No spam. Unsubscribe any time.\nBrowse Topics\nGuided Learning Paths\nBasics\nIntermediate\nAdvanced\nai\napi\nbest-practices\ncareer\ncommunity\ndatabases\ndata-science\ndata-structures\ndata-viz\ndevops\ndjango\ndocker\neditors\nflask\nfront-end\ngamedev\ngui\nmachine-learning\nnews\nnumpy\nprojects\npython\ntesting\ntools\nweb-dev\nweb-scraping\nTable of Contents\nScrape and Parse Text From Websites\nBuild Your First Web Scraper\nExtract Text From HTML With String Methods\nGet to Know Regular Expressions\nExtract Text From HTML With Regular Expressions\nCheck Your Understanding\nUse an HTML Parser for Web Scraping in Python\nInstall Beautiful Soup\nCreate a BeautifulSoup Object\nUse a BeautifulSoup Object\nCheck Your Understanding\nInteract With HTML Forms\nInstall MechanicalSoup\nCreate a Browser Object\nSubmit a Form With MechanicalSoup\nCheck Your Understanding\nInteract With Websites in Real Time\nConclusion\nAdditional Resources\nFrequently Asked Questions\nMark as Completed\nShare\nRecommended Video Course\nIntroduction to Web Scraping With Python\nA Practical Introduction to Web Scraping in Python\nby\nDavid Amos\nPublication date\nDec 21, 2024\nReading time estimate\n38m\nintermediate\nweb-scraping\nMark as Completed\nShare\nTable of Contents\nScrape and Parse Text From Websites\nBuild Your First Web Scraper\nExtract Text From HTML With String Methods\nGet to Know Regular Expressions\nExtract Text From HTML With Regular Expressions\nCheck Your Understanding\nUse an HTML Parser for Web Scraping in Python\nInstall Beautiful Soup\nCreate a BeautifulSoup Object\nUse a BeautifulSoup Object\nCheck Your Understanding\nInteract With HTML Forms\nInstall MechanicalSoup\nCreate a Browser Object\nSubmit a Form With MechanicalSoup\nCheck Your Understanding\nInteract With Websites in Real Time\nConclusion\nAdditional Resources\nFrequently Asked Questions\nRemove ads\nWatch Now\nThis tutorial has a related video course created by the Real Python team. Watch it together with the written tutorial to deepen your understanding:\nIntroduction to Web Scraping With Python\nPython web scraping allows you to collect and parse data from websites programmatically. With powerful libraries like\nurllib\n, Beautiful Soup, and MechanicalSoup, you can fetch and manipulate HTML content effortlessly. By automating data collection tasks, Python makes web scraping both efficient and effective.\nBy the end of this tutorial, you\u2019ll understand that:\nPython is\nwell-suited for web scraping\ndue to its\nextensive libraries\n, such as Beautiful Soup and MechanicalSoup.\nYou can scrape websites with Python by\nfetching HTML content\nusing\nurllib\nand\nextracting data\nusing string methods or parsers like Beautiful Soup.\nBeautiful Soup\nis a great choice for\nparsing HTML\ndocuments with Python effectively.\nData scraping may be illegal\nif it violates a website\u2019s terms of use, so always review the website\u2019s acceptable use policy.\nThis tutorial guides you through extracting data from websites using string methods, regular expressions, and HTML parsers.\nNote:\nThis tutorial is adapted from the chapter \u201cInteracting With the Web\u201d in\nPython Basics: A Practical Introduction to Python 3\n.\nThe book uses Python\u2019s built-in\nIDLE\neditor to create and edit Python files and interact with the Python shell, so you\u2019ll see occasional references to IDLE throughout this tutorial. However, you should have no problems running the example code from the\neditor\nand\nenvironment\nof your choice.\nSource Code:\nClick here to download the free source code\nthat you\u2019ll use to collect and parse data from the Web.\nTake the Quiz:\nTest your knowledge with our interactive \u201cA Practical Introduction to Web Scraping in Python\u201d quiz. You\u2019ll receive a score upon completion to help you track your learning progress:\nInteractive Quiz\nA Practical Introduction to Web Scraping in Python\nIn this quiz, you'll test your understanding of web scraping in Python. Web scraping is a powerful tool for data collection and analysis. By working through this quiz, you'll revisit how to parse website data using string methods, regular expressions, and HTML parsers, as well as how to interact with forms and other website components.\nScrape and Parse Text From Websites\nCollecting data from websites using an automated process is known as web scraping. Some websites explicitly forbid users from scraping their data with automated tools like the ones that you\u2019ll create in this tutorial. Websites do this for two possible reasons:\nThe site has a good reason to protect its data. For instance, Google Maps doesn\u2019t let you request too many results too quickly.\nMaking many repeated requests to a website\u2019s server may use up bandwidth, slowing down the website for other users and potentially overloading the server such that the website stops responding entirely.\nBefore using your Python skills for web scraping, you should always check your target website\u2019s acceptable use policy to see if accessing the website with automated tools is a violation of its terms of use. Legally, web scraping against the wishes of a website is very much a gray area.\nImportant:\nPlease be aware that the following techniques\nmay be illegal\nwhen used on websites that prohibit web scraping.\nFor this tutorial, you\u2019ll use a page that\u2019s hosted on Real Python\u2019s server. The page that you\u2019ll access has been set up for use with this tutorial.\nNow that you\u2019ve read the disclaimer, you can get to the fun stuff. In the next section, you\u2019ll start grabbing all the HTML code from a single web page.\nRemove ads\nBuild Your First Web Scraper\nOne useful package for web scraping that you can find in Python\u2019s\nstandard library\nis\nurllib\n, which contains tools for working with URLs. In particular, the\nurllib.request\nmodule contains a function called\nurlopen()\nthat you can use to open a URL within a program.\nIn IDLE\u2019s interactive window, type the following to import\nurlopen()\n:\nPython\n>>>\nfrom\nurllib.request\nimport\nurlopen\nThe web page that you\u2019ll open is at the following URL:\nPython\n>>>\nurl\n=\n\"http://olympus.realpython.org/profiles/aphrodite\"\nTo open the web page, pass\nurl\nto\nurlopen()\n:\nPython\n>>>\npage\n=\nurlopen\n(\nurl\n)\nurlopen()\nreturns an\nHTTPResponse\nobject:\nPython\n>>>\npage\n<http.client.HTTPResponse object at 0x105fef820>\nTo extract the HTML from the page, first use the\nHTTPResponse\nobject\u2019s\n.read()\nmethod, which returns a sequence of bytes. Then use\n.decode()\nto decode the bytes to a string using\nUTF-8\n:\nPython\n>>>\nhtml_bytes\n=\npage\n.\nread\n()\n>>>\nhtml\n=\nhtml_bytes\n.\ndecode\n(\n\"utf-8\"\n)\nNow you can\nprint\nthe HTML to see the contents of the web page:\nPython\n>>>\nprint\n(\nhtml\n)\n<html>\n<head>\n<title>Profile: Aphrodite</title>\n</head>\n<body bgcolor=\"yellow\">\n<center>\n<br><br>\n<img src=\"/static/aphrodite.gif\" />\n<h2>Name: Aphrodite</h2>\n<br><br>\nFavorite animal: Dove\n<br><br>\nFavorite color: Red\n<br><br>\nHometown: Mount Olympus\n</center>\n</body>\n</html>\nThe output that you\u2019re seeing is the\nHTML code\nof the website, which your browser renders when you visit\nhttp://olympus.realpython.org/profiles/aphrodite\n:\nWith\nurllib\n, you accessed the website similarly to how you would in your browser. However, instead of rendering the content visually, you grabbed the source code as text. Now that you have the HTML as text, you can extract information from it in a couple of different ways.\nExtract Text From HTML With String Methods\nOne way to extract information from a web page\u2019s HTML is to use\nstring methods\n. For instance, you can use\n.find()\nto search through the text of the HTML for the\n<title>\ntags and extract the title of the web page.\nTo start, you\u2019ll extract the title of the web page that you requested in the previous example. If you know the index of the first character of the title and the index of the first character of the closing\n</title>\ntag, then you can use a\nstring slice\nto extract the title.\nBecause\n.find()\nreturns the index of the first occurrence of a\nsubstring\n, you can get the index of the opening\n<title>\ntag by passing the string\n\"<title>\"\nto\n.find()\n:\nPython\n>>>\ntitle_index\n=\nhtml\n.\nfind\n(\n\"<title>\"\n)\n>>>\ntitle_index\n14\nYou don\u2019t want the index of the\n<title>\ntag, though. You want the index of the title itself. To get the index of the first letter in the title, you can add the length of the string\n\"<title>\"\nto\ntitle_index\n:\nPython\n>>>\nstart_index\n=\ntitle_index\n+\nlen\n(\n\"<title>\"\n)\n>>>\nstart_index\n21\nNow get the index of the closing\n</title>\ntag by passing the string\n\"</title>\"\nto\n.find()\n:\nPython\n>>>\nend_index\n=\nhtml\n.\nfind\n(\n\"</title>\"\n)\n>>>\nend_index\n39\nFinally, you can extract the title by slicing the\nhtml\nstring:\nPython\n>>>\ntitle\n=\nhtml\n[\nstart_index\n:\nend_index\n]\n>>>\ntitle\n'Profile: Aphrodite'\nReal-world HTML can be much more complicated and far less predictable than the HTML on the Aphrodite profile page. Here\u2019s\nanother profile page\nwith some messier HTML that you can scrape:\nPython\n>>>\nurl\n=\n\"http://olympus.realpython.org/profiles/poseidon\"\nTry extracting the title from this new URL using the same method as in the previous example:\nPython\n>>>\nurl\n=\n\"http://olympus.realpython.org/profiles/poseidon\"\n>>>\npage\n=\nurlopen\n(\nurl\n)\n>>>\nhtml\n=\npage\n.\nread\n()\n.\ndecode\n(\n\"utf-8\"\n)\n>>>\nstart_index\n=\nhtml\n.\nfind\n(\n\"<title>\"\n)\n+\nlen\n(\n\"<title>\"\n)\n>>>\nend_index\n=\nhtml\n.\nfind\n(\n\"</title>\"\n)\n>>>\ntitle\n=\nhtml\n[\nstart_index\n:\nend_index\n]\n>>>\ntitle\n'\\n<head>\\n<title >Profile: Poseidon'\nWhoops! There\u2019s a bit of HTML mixed in with the title. Why\u2019s that?\nThe HTML for the\n/profiles/poseidon\npage looks similar to the\n/profiles/aphrodite\npage, but there\u2019s a small difference. The opening\n<title>\ntag has an extra space before the closing angle bracket (\n>\n), rendering it as\n<title >\n.\nhtml.find(\"<title>\")\nreturns\n-1\nbecause the exact substring\n\"<title>\"\ndoesn\u2019t exist. When\n-1\nis added to\nlen(\"<title>\")\n, which is\n7\n, the\nstart_index\nvariable is assigned the value\n6\n.\nThe character at index\n6\nof the string\nhtml\nis a newline character (\n\\n\n) right before the opening angle bracket (\n<\n) of the\n<head>\ntag. This means that\nhtml[start_index:end_index]\nreturns all the HTML starting with that newline and ending just before the\n</title>\ntag.\nThese sorts of problems can occur in countless unpredictable ways. You need a more reliable way to extract text from HTML.\nRemove ads\nGet to Know Regular Expressions\nRegular expressions\n\u2014or\nregexes\nfor short\u2014are patterns that you can use to search for text within a string. Python supports regular expressions through the standard library\u2019s\nre\nmodule.\nNote:\nRegular expressions aren\u2019t particular to Python. They\u2019re a general programming concept and are supported in many programming languages.\nTo work with regular expressions, the first thing that you need to do is import the\nre\nmodule:\nPython\n>>>\nimport\nre\nRegular expressions use special characters called\nmetacharacters\nto denote different patterns. For instance, the asterisk character (\n*\n) stands for zero or more instances of whatever comes just before the asterisk.\nIn the following example, you use\n.findall()\nto find any text within a string that matches a given regular expression:\nPython\n>>>\nre\n.\nfindall\n(\n\"ab*c\"\n,\n\"ac\"\n)\n['ac']\nThe first argument of\nre.findall()\nis the regular expression that you want to match, and the second argument is the string to test. In the above example, you search for the pattern\n\"ab*c\"\nin the string\n\"ac\"\n.\nThe regular expression\n\"ab*c\"\nmatches any part of the string that begins with\n\"a\"\n, ends with\n\"c\"\n, and has zero or more instances of\n\"b\"\nbetween the two.\nre.findall()\nreturns a\nlist\nof all matches. The string\n\"ac\"\nmatches this pattern, so it\u2019s returned in the list.\nHere\u2019s the same pattern applied to different strings:\nPython\n>>>\nre\n.\nfindall\n(\n\"ab*c\"\n,\n\"abcd\"\n)\n['abc']\n>>>\nre\n.\nfindall\n(\n\"ab*c\"\n,\n\"acc\"\n)\n['ac']\n>>>\nre\n.\nfindall\n(\n\"ab*c\"\n,\n\"abcac\"\n)\n['abc', 'ac']\n>>>\nre\n.\nfindall\n(\n\"ab*c\"\n,\n\"abdc\"\n)\n[]\nNotice that if no match is found, then\n.findall()\nreturns an empty list.\nPattern matching is case sensitive. If you want to match this pattern regardless of the case, then you can pass a third argument with the value\nre.IGNORECASE\n:\nPython\n>>>\nre\n.\nfindall\n(\n\"ab*c\"\n,\n\"ABC\"\n)\n[]\n>>>\nre\n.\nfindall\n(\n\"ab*c\"\n,\n\"ABC\"\n,\nre\n.\nIGNORECASE\n)\n['ABC']\nYou can use a period (\n.\n) to stand for any single character in a regular expression. For instance, you could find all the strings that contain the letters\n\"a\"\nand\n\"c\"\nseparated by a single character as follows:\nPython\n>>>\nre\n.\nfindall\n(\n\"a.c\"\n,\n\"abc\"\n)\n['abc']\n>>>\nre\n.\nfindall\n(\n\"a.c\"\n,\n\"abbc\"\n)\n[]\n>>>\nre\n.\nfindall\n(\n\"a.c\"\n,\n\"ac\"\n)\n[]\n>>>\nre\n.\nfindall\n(\n\"a.c\"\n,\n\"acc\"\n)\n['acc']\nThe pattern\n.*\ninside a regular expression stands for any character repeated any number of times. For instance, you can use\n\"a.*c\"\nto find every substring that starts with\n\"a\"\nand ends with\n\"c\"\n, regardless of which letter\u2014or letters\u2014are in between:\nPython\n>>>\nre\n.\nfindall\n(\n\"a.*c\"\n,\n\"abc\"\n)\n['abc']\n>>>\nre\n.\nfindall\n(\n\"a.*c\"\n,\n\"abbc\"\n)\n['abbc']\n>>>\nre\n.\nfindall\n(\n\"a.*c\"\n,\n\"ac\"\n)\n['ac']\n>>>\nre\n.\nfindall\n(\n\"a.*c\"\n,\n\"acc\"\n)\n['acc']\nOften, you use\nre.search()\nto search for a particular pattern inside a string. This function is somewhat more complicated than\nre.findall()\nbecause it returns an object called\nMatchObject\nthat stores different groups of data. This is because there might be matches inside other matches, and\nre.search()\nreturns every possible result.\nThe details of\nMatchObject\nare irrelevant here. For now, just know that calling\n.group()\non\nMatchObject\nwill return the first and most inclusive result, which in most cases is just what you want:\nPython\n>>>\nmatch_results\n=\nre\n.\nsearch\n(\n\"ab*c\"\n,\n\"ABC\"\n,\nre\n.\nIGNORECASE\n)\n>>>\nmatch_results\n.\ngroup\n()\n'ABC'\nThere\u2019s one more function in the\nre\nmodule that\u2019s useful for parsing out text.\nre.sub()\n, which is short for\nsubstitute\n, allows you to replace the text in a string that matches a regular expression with new text. It behaves sort of like the\n.replace()\nstring method.\nThe arguments passed to\nre.sub()\nare the regular expression, followed by the replacement text, followed by the string. Here\u2019s an example:\nPython\n>>>\nstring\n=\n\"Everything is <replaced> if it's in <tags>.\"\n>>>\nstring\n=\nre\n.\nsub\n(\n\"<.*>\"\n,\n\"ELEPHANTS\"\n,\nstring\n)\n>>>\nstring\n'Everything is ELEPHANTS.'\nPerhaps that wasn\u2019t quite what you expected to happen.\nre.sub()\nuses the regular expression\n\"<.*>\"\nto find and replace everything between the first\n<\nand the last\n>\n, which spans from the beginning of\n<replaced>\nto the end of\n<tags>\n. This is because Python\u2019s regular expressions are\ngreedy\n, meaning they try to find the longest possible match when characters like\n*\nare used.\nAlternatively, you can use the non-greedy matching pattern\n*?\n, which works the same way as\n*\nexcept that it matches the shortest possible string of text:\nPython\n>>>\nstring\n=\n\"Everything is <replaced> if it's in <tags>.\"\n>>>\nstring\n=\nre\n.\nsub\n(\n\"<.*?>\"\n,\n\"ELEPHANTS\"\n,\nstring\n)\n>>>\nstring\n\"Everything is ELEPHANTS if it's in ELEPHANTS.\"\nThis time,\nre.sub()\nfinds two matches,\n<replaced>\nand\n<tags>\n, and substitutes the string\n\"ELEPHANTS\"\nfor both matches.\nRemove ads\nExtract Text From HTML With Regular Expressions\nEquipped with all this knowledge, now try to parse out the title from\nanother profile page\n, which includes this rather carelessly written line of HTML:\nHTML\n<\nTITLE\n>\nProfile: Dionysus\n<\n/title / >\nThe\n.find()\nmethod would have a difficult time dealing with the inconsistencies here, but with the clever use of regular expressions, you can handle this code quickly and efficiently:\nPython\nregex_soup.py\nimport\nre\nfrom\nurllib.request\nimport\nurlopen\nurl\n=\n\"http://olympus.realpython.org/profiles/dionysus\"\npage\n=\nurlopen\n(\nurl\n)\nhtml\n=\npage\n.\nread\n()\n.\ndecode\n(\n\"utf-8\"\n)\npattern\n=\n\"<title.*?>.*?</title.*?>\"\nmatch_results\n=\nre\n.\nsearch\n(\npattern\n,\nhtml\n,\nre\n.\nIGNORECASE\n)\ntitle\n=\nmatch_results\n.\ngroup\n()\ntitle\n=\nre\n.\nsub\n(\n\"<.*?>\"\n,\n\"\"\n,\ntitle\n)\n# Remove HTML tags\nprint\n(\ntitle\n)\nTake a closer look at the first regular expression in the\npattern\nstring by breaking it down into three parts:\n<title.*?>\nmatches the opening\n<TITLE >\ntag in\nhtml\n. The\n<title\npart of the pattern matches with\n<TITLE\nbecause\nre.search()\nis called with\nre.IGNORECASE\n, and\n.*?>\nmatches any text after\n<TITLE\nup to the first instance of\n>\n.\n.*?\nnon-greedily matches all text after the opening\n<TITLE >\n, stopping at the first match for\n</title.*?>\n.\n</title.*?>\ndiffers from the first pattern only in its use of the\n/\ncharacter, so it matches the closing\n</title / >\ntag in\nhtml\n.\nThe second regular expression, the string\n\"<.*?>\"\n, also uses the non-greedy\n.*?\nto match all the HTML tags in the\ntitle\nstring. By replacing any matches with\n\"\"\n,\nre.sub()\nremoves all the tags and returns only the text.\nNote:\nWeb scraping in Python or any other language can be tedious. No two websites are organized the same way, and HTML is often messy. Moreover, websites change over time. Web scrapers that work today aren\u2019t guaranteed to work next year\u2014or next week, for that matter!\nRegular expressions are a powerful tool when used correctly. In this introduction, you\u2019ve barely scratched the surface. For more about regular expressions and how to use them, check out the two-part series\nRegular Expressions: Regexes in Python\n.\nCheck Your Understanding\nExpand the block below to check your understanding.\nExercise: Scrape Data From a Website\nShow/Hide\nWrite a program that grabs the full HTML from the following URL:\nPython\n>>>\nurl\n=\n\"http://olympus.realpython.org/profiles/dionysus\"\nThen use\n.find()\nto display the text following\nName:\nand\nFavorite Color:\n(not including any leading spaces or trailing HTML tags that might appear on the same line).\nYou can expand the block below to see a solution.\nSolution: Scrape Data From a Website\nShow/Hide\nFirst, import the\nurlopen\nfunction from the\nurlib.request\nmodule:\nPython\nfrom\nurllib.request\nimport\nurlopen\nThen open the URL and use the\n.read()\nmethod of the\nHTTPResponse\nobject returned by\nurlopen()\nto read the page\u2019s HTML:\nPython\nurl\n=\n\"http://olympus.realpython.org/profiles/dionysus\"\nhtml_page\n=\nurlopen\n(\nurl\n)\nhtml_text\n=\nhtml_page\n.\nread\n()\n.\ndecode\n(\n\"utf-8\"\n)\nThe\n.read()\nmethod returns a byte string, so you use\n.decode()\nto decode the bytes using the UTF-8 encoding.\nNow that you have the HTML source of the web page as a string assigned to the\nhtml_text\nvariable, you can extract Dionysus\u2019s name and favorite color from his profile. The structure of the HTML for Dionysus\u2019s profile is the same as for Aphrodite\u2019s profile, which you saw earlier.\nYou can get the name by finding the string\n\"Name:\"\nin the text and extracting everything that comes after the first occurence of the string and before the next HTML tag. That is, you need to extract everything after the colon (\n:\n) and before the first angle bracket (\n<\n). You can use the same technique to extract the favorite color.\nThe following\nfor\nloop\nextracts this text for both the name and favorite color:\nPython\nfor\nstring\nin\n[\n\"Name: \"\n,\n\"Favorite Color:\"\n]:\nstring_start_idx\n=\nhtml_text\n.\nfind\n(\nstring\n)\ntext_start_idx\n=\nstring_start_idx\n+\nlen\n(\nstring\n)\nnext_html_tag_offset\n=\nhtml_text\n[\ntext_start_idx\n:]\n.\nfind\n(\n\"<\"\n)\ntext_end_idx\n=\ntext_start_idx\n+\nnext_html_tag_offset\nraw_text\n=\nhtml_text\n[\ntext_start_idx\n:\ntext_end_idx\n]\nclean_text\n=\nraw_text\n.\nstrip\n(\n\"\n\\r\\n\\t\n\"\n)\nprint\n(\nclean_text\n)\nIt looks like there\u2019s a lot going on in this\nfor\nloop, but it\u2019s just a little bit of arithmetic to calculate the right indices for extracting the desired text. Go ahead and break it down:\nYou use\nhtml_text.find()\nto find the starting index of the string, either\n\"Name:\"\nor\n\"Favorite Color:\"\n, and then assign the index to\nstring_start_idx\n.\nSince the text to extract starts just after the colon in\n\"Name:\"\nor\n\"Favorite Color:\"\n, you get the index of the character immediately after the colon by adding the length of the string to\nstart_string_idx\n, and then assign the result to\ntext_start_idx\n.\nYou calculate the ending index of the text to extract by determining the index of the first angle bracket (\n<\n) relative to\ntext_start_idx\nand assign this value to\nnext_html_tag_offset\n. Then you add that value to\ntext_start_idx\nand assign the result to\ntext_end_idx\n.\nYou extract the text by slicing\nhtml_text\nfrom\ntext_start_idx\nto\ntext_end_idx\nand assign this string to\nraw_text\n.\nYou remove any whitespace from the beginning and end of\nraw_text\nusing\n.strip()\nand assign the result to\nclean_text\n.\nAt the end of the loop, you use\nprint()\nto display the extracted text. The final output looks like this:\nShell\nDionysus\nWine\nThis solution is one of many that solves this problem, so if you got the same output with a different solution, then you did great!\nWhen you\u2019re ready, you can move on to the next section.\nUse an HTML Parser for Web Scraping in Python\nAlthough regular expressions are great for pattern matching in general, sometimes it\u2019s easier to use an HTML parser that\u2019s explicitly designed for parsing out HTML pages. There are many Python tools written for this purpose, but the\nBeautiful Soup\nlibrary is a good one to start with.\nInstall Beautiful Soup\nTo install Beautiful Soup, you can run the following in your\nterminal\n:\nShell\n$\npython\n-m\npip\ninstall\nbeautifulsoup4\nWith this command, you\u2019re installing the latest version of Beautiful Soup into your global Python environment.\nRemove ads\nCreate a\nBeautifulSoup\nObject\nType the following program into a new editor window:\nPython\nbeauty_soup.py\nfrom\nbs4\nimport\nBeautifulSoup\nfrom\nurllib.request\nimport\nurlopen\nurl\n=\n\"http://olympus.realpython.org/profiles/dionysus\"\npage\n=\nurlopen\n(\nurl\n)\nhtml\n=\npage\n.\nread\n()\n.\ndecode\n(\n\"utf-8\"\n)\nsoup\n=\nBeautifulSoup\n(\nhtml\n,\n\"html.parser\"\n)\nThis program does three things:\nOpens the URL\nhttp://olympus.realpython.org/profiles/dionysus\nby using\nurlopen()\nfrom the\nurllib.request\nmodule\nReads the HTML from the page as a string and assigns it to the\nhtml\nvariable\nCreates a\nBeautifulSoup\nobject and assigns it to the\nsoup\nvariable\nThe\nBeautifulSoup\nobject assigned to\nsoup\nis created with two arguments. The first argument is the HTML to be parsed, and the second argument, the string\n\"html.parser\"\n, tells the object which parser to use behind the scenes.\n\"html.parser\"\nrepresents Python\u2019s built-in HTML parser.\nUse a\nBeautifulSoup\nObject\nSave and run the above program. When it\u2019s finished running, you can use the\nsoup\nvariable in the interactive window to parse the content of\nhtml\nin various ways.\nNote:\nIf you\u2019re not using IDLE, then you can run your program with the\n-i\nflag to enter interactive mode. Something like\npython -i beauty_soup.py\nwill first run your program and then leave you in a REPL where you can explore your objects.\nFor example,\nBeautifulSoup\nobjects have a\n.get_text()\nmethod that you can use to extract all the text from the document and automatically remove any HTML tags.\nType the following code into IDLE\u2019s interactive window or at the end of the code in your editor:\nPython\n>>>\nprint\n(\nsoup\n.\nget_text\n())\nProfile: Dionysus\nName: Dionysus\nHometown: Mount Olympus\nFavorite animal: Leopard\nFavorite Color: Wine\nThere are a lot of blank lines in this output. These are the result of newline characters in the HTML document\u2019s text. You can remove them with the\n.replace()\nstring method if you need to.\nOften, you need to get only specific text from an HTML document. Using Beautiful Soup first to extract the text and then using the\n.find()\nstring method is sometimes easier than working with regular expressions.\nHowever, other times the HTML tags themselves are the elements that point out the data you want to retrieve. For instance, perhaps you want to retrieve the URLs for all the images on the page. These links are contained in the\nsrc\nattribute of\n<img>\nHTML tags.\nIn this case, you can use\nfind_all()\nto return a list of all instances of that particular tag:\nPython\n>>>\nsoup\n.\nfind_all\n(\n\"img\"\n)\n[<img src=\"/static/dionysus.jpg\"/>, <img src=\"/static/grapes.png\"/>]\nThis returns a list of all\n<img>\ntags in the HTML document. The objects in the list look like they might be strings representing the tags, but they\u2019re actually instances of the\nTag\nobject provided by Beautiful Soup.\nTag\nobjects provide a simple interface for working with the information they contain.\nYou can explore this a little by first unpacking the\nTag\nobjects from the list:\nPython\n>>>\nimage1\n,\nimage2\n=\nsoup\n.\nfind_all\n(\n\"img\"\n)\nEach\nTag\nobject has a\n.name\nproperty that returns a string containing the HTML tag type:\nPython\n>>>\nimage1\n.\nname\n'img'\nYou can access the HTML attributes of the\nTag\nobject by putting their names between square brackets, just as if the attributes were keys in a dictionary.\nFor example, the\n<img src=\"/static/dionysus.jpg\"/>\ntag has a single attribute,\nsrc\n, with the value\n\"/static/dionysus.jpg\"\n. Likewise, an HTML tag such as the link\n<a href=\"https://realpython.com\" target=\"_blank\">\nhas two attributes,\nhref\nand\ntarget\n.\nTo get the source of the images in the Dionysus profile page, you access the\nsrc\nattribute using the dictionary notation mentioned above:\nPython\n>>>\nimage1\n[\n\"src\"\n]\n'/static/dionysus.jpg'\n>>>\nimage2\n[\n\"src\"\n]\n'/static/grapes.png'\nCertain tags in HTML documents can be accessed by properties of the\nTag\nobject. For example, to get the\n<title>\ntag in a document, you can use the\n.title\nproperty:\nPython\n>>>\nsoup\n.\ntitle\n<title>Profile: Dionysus</title>\nIf you look at the source of the Dionysus profile by navigating to the\nprofile page\n, right-clicking on the page, and selecting\nView page source\n, then you\u2019ll notice that the\n<title>\ntag is written in all caps with spaces:\nBeautiful Soup automatically cleans up the tags for you by removing the extra space in the opening tag and the extraneous forward slash (\n/\n) in the closing tag.\nYou can also retrieve just the string between the title tags with the\n.string\nproperty of the\nTag\nobject:\nPython\n>>>\nsoup\n.\ntitle\n.\nstring\n'Profile: Dionysus'\nOne of the features of Beautiful Soup is the ability to search for specific kinds of tags whose attributes match certain values. For example, if you want to find all the\n<img>\ntags that have a\nsrc\nattribute equal to the value\n/static/dionysus.jpg\n, then you can provide the following additional argument to\n.find_all()\n:\nPython\n>>>\nsoup\n.\nfind_all\n(\n\"img\"\n,\nsrc\n=\n\"/static/dionysus.jpg\"\n)\n[<img src=\"/static/dionysus.jpg\"/>]\nThis example is somewhat arbitrary, and the usefulness of this technique may not be apparent from the example. If you spend some time browsing various websites and viewing their page sources, then you\u2019ll notice that many websites have extremely complicated HTML structures.\nWhen scraping data from websites with Python, you\u2019re often interested in particular parts of the page. By spending some time looking through the HTML document, you can identify tags with unique attributes that you can use to extract the data you need.\nThen, instead of relying on complicated regular expressions or using\n.find()\nto search through the document, you can directly access the particular tag that you\u2019re interested in and extract the data you need.\nIn some cases, you may find that Beautiful Soup doesn\u2019t offer the functionality you need. The\nlxml\nlibrary is somewhat trickier to get started with but offers far more flexibility than Beautiful Soup for parsing HTML documents. You may want to check it out once you\u2019re comfortable using Beautiful Soup.\nNote:\nHTML parsers like Beautiful Soup can save you a lot of time and effort when it comes to locating specific data in web pages. However, sometimes HTML is so poorly written and disorganized that even a sophisticated parser like Beautiful Soup can\u2019t interpret the HTML tags properly.\nIn this case, you\u2019re often left with using\n.find()\nand regular expression techniques to try to parse out the information that you need.\nBeautiful Soup is great for scraping data from a website\u2019s HTML, but it doesn\u2019t provide any way to work with HTML forms. For example, if you need to search a website for some query and then scrape the results, then Beautiful Soup alone won\u2019t get you very far.\nRemove ads\nCheck Your Understanding\nExpand the block below to check your understanding.\nExercise: Parse HTML With Beautiful Soup\nShow/Hide\nWrite a program that grabs the full HTML from the\npage\nat the URL\nhttp://olympus.realpython.org/profiles\n.\nUsing Beautiful Soup, print out a list of all the links on the page by looking for HTML tags with the name\na\nand retrieving the value taken on by the\nhref\nattribute of each tag.\nThe final output should look like this:\nShell\nhttp://olympus.realpython.org/profiles/aphrodite\nhttp://olympus.realpython.org/profiles/poseidon\nhttp://olympus.realpython.org/profiles/dionysus\nMake sure that you only have one slash (\n/\n) between the base URL and the relative URL.\nYou can expand the block below to see a solution:\nSolution: Parse HTML With Beautiful Soup\nShow/Hide\nFirst, import the\nurlopen\nfunction from the\nurlib.request\nmodule and the\nBeautifulSoup\nclass from the\nbs4\npackage:\nPython\nfrom\nurllib.request\nimport\nurlopen\nfrom\nbs4\nimport\nBeautifulSoup\nEach link URL on the\n/profiles\npage is a\nrelative URL\n, so create a\nbase_url\nvariable with the base URL of the website:\nPython\nbase_url\n=\n\"http://olympus.realpython.org\"\nYou can build a full URL by concatenating\nbase_url\nwith a relative URL.\nNow open the\n/profiles\npage with\nurlopen()\nand use\n.read()\nto get the HTML source:\nPython\nhtml_page\n=\nurlopen\n(\nbase_url\n+\n\"/profiles\"\n)\nhtml_text\n=\nhtml_page\n.\nread\n()\n.\ndecode\n(\n\"utf-8\"\n)\nWith the HTML source downloaded and decoded, you can create a new\nBeautifulSoup\nobject to parse the HTML:\nPython\nsoup\n=\nBeautifulSoup\n(\nhtml_text\n,\n\"html.parser\"\n)\nsoup.find_all(\"a\")\nreturns a list of all the links in the HTML source. You can loop over this list to print out all the links on the web page:\nPython\nfor\nlink\nin\nsoup\n.\nfind_all\n(\n\"a\"\n):\nlink_url\n=\nbase_url\n+\nlink\n[\n\"href\"\n]\nprint\n(\nlink_url\n)\nYou can access the relative URL for each link through the\n\"href\"\nsubscript. Concatenate this value with\nbase_url\nto create the full\nlink_url\n.\nWhen you\u2019re ready, you can move on to the next section.\nInteract With HTML Forms\nThe\nurllib\nmodule that you\u2019ve been working with so far in this tutorial is well suited for requesting the contents of a web page. Sometimes, though, you need to interact with a web page to obtain the content you need. For example, you might need to submit a form or click a button to display hidden content.\nNote:\nThis tutorial is adapted from the chapter \u201cInteracting With the Web\u201d in\nPython Basics: A Practical Introduction to Python 3\n. If you enjoy what you\u2019re reading, then be sure to check out\nthe rest of the book\n.\nThe Python standard library doesn\u2019t provide a built-in means for working with web pages interactively, but many third-party packages are available from\nPyPI\n. Among these,\nMechanicalSoup\nis a popular and relatively straightforward package to use.\nIn essence, MechanicalSoup installs what\u2019s known as a\nheadless browser\n, which is a web browser with no graphical user interface. This browser is controlled programmatically via a Python program.\nInstall MechanicalSoup\nYou can install MechanicalSoup with\npip\nin your terminal:\nShell\n$\npython\n-m\npip\ninstall\nMechanicalSoup\nYou\u2019ll need to close and restart your IDLE session for MechanicalSoup to load and be recognized after it\u2019s been installed.\nCreate a\nBrowser\nObject\nType the following into IDLE\u2019s interactive window:\nPython\n>>>\nimport\nmechanicalsoup\n>>>\nbrowser\n=\nmechanicalsoup\n.\nBrowser\n()\nBrowser\nobjects represent the headless web browser. You can use them to request a page from the Internet by passing a URL to their\n.get()\nmethod:\nPython\n>>>\nurl\n=\n\"http://olympus.realpython.org/login\"\n>>>\npage\n=\nbrowser\n.\nget\n(\nurl\n)\npage\nis a\nResponse\nobject that stores the response from requesting the URL from the browser:\nPython\n>>>\npage\n<Response [200]>\nThe number\n200\nrepresents the\nstatus code\nreturned by the request. A status code of\n200\nmeans that the request was successful. An unsuccessful request might show a status code of\n404\nif the URL doesn\u2019t exist or\n500\nif there\u2019s a server error when making the request.\nMechanicalSoup uses Beautiful Soup to parse the HTML from the request, and\npage\nhas a\n.soup\nattribute that represents a\nBeautifulSoup\nobject:\nPython\n>>>\ntype\n(\npage\n.\nsoup\n)\n<class 'bs4.BeautifulSoup'>\nYou can view the HTML by inspecting the\n.soup\nattribute:\nPython\n>>>\npage\n.\nsoup\n<html>\n<head>\n<title>Log In</title>\n</head>\n<body bgcolor=\"yellow\">\n<center>\n<br/><br/>\n<h2>Please log in to access Mount Olympus:</h2>\n<br/><br/>\n<form action=\"/login\" method=\"post\" name=\"login\">\nUsername: <input name=\"user\" type=\"text\"/><br/>\nPassword: <input name=\"pwd\" type=\"password\"/><br/><br/>\n<input type=\"submit\" value=\"Submit\"/>\n</form>\n</center>\n</body>\n</html>\nNotice this page has a\n<form>\non it with\n<input>\nelements for a username and a password.\nRemove ads\nSubmit a Form With MechanicalSoup\nOpen the\n/login\npage from the previous example in a browser and look at it yourself before moving on:\nTry typing in a random username and password combination. If you guess incorrectly, then the message\nWrong username or password!\nis displayed at the bottom of the page.\nHowever, if you provide the correct login credentials, then you\u2019re redirected to the\n/profiles\npage:\nUsername\nPassword\nzeus\nThunderDude\nIn the next example, you\u2019ll see how to use MechanicalSoup to fill out and submit this form using Python!\nThe important section of HTML code is the login form\u2014that is, everything inside the\n<form>\ntags. The\n<form>\non this page has the\nname\nattribute set to\nlogin\n. This form contains two\n<input>\nelements, one named\nuser\nand the other named\npwd\n. The third\n<input>\nelement is the\nSubmit\nbutton.\nNow that you know the underlying structure of the login form, as well as the credentials needed to log in, take a look at a program that fills the form out and submits it.\nIn a new editor window, type in the following program:\nPython\nimport\nmechanicalsoup\n# 1\nbrowser\n=\nmechanicalsoup\n.\nBrowser\n()\nurl\n=\n\"http://olympus.realpython.org/login\"\nlogin_page\n=\nbrowser\n.\nget\n(\nurl\n)\nlogin_html\n=\nlogin_page\n.\nsoup\n# 2\nform\n=\nlogin_html\n.\nselect\n(\n\"form\"\n)[\n0\n]\nform\n.\nselect\n(\n\"input\"\n)[\n0\n][\n\"value\"\n]\n=\n\"zeus\"\nform\n.\nselect\n(\n\"input\"\n)[\n1\n][\n\"value\"\n]\n=\n\"ThunderDude\"\n# 3\nprofiles_page\n=\nbrowser\n.\nsubmit\n(\nform\n,\nlogin_page\n.\nurl\n)\nSave the file and press\nF5\nto run it. To confirm that you\u2019ve successfully logged in, type the following into the interactive window:\nPython\n>>>\nprofiles_page\n.\nurl\n'http://olympus.realpython.org/profiles'\nNow break down the above example:\nYou create a\nBrowser\ninstance and use it to request the URL\nhttp://olympus.realpython.org/login\n. You assign the HTML content of the page to the\nlogin_html\nvariable using the\n.soup\nproperty.\nlogin_html.select(\"form\")\nreturns a list of all\n<form>\nelements on the page. Because the page has only one\n<form>\nelement, you can access the form by retrieving the element at index\n0\nof the list. When there is only one form on a page, you may also use\nlogin_html.form\n. The next two lines select the username and password inputs and set their value to\n\"zeus\"\nand\n\"ThunderDude\"\n, respectively.\nYou submit the form with\nbrowser.submit()\n. Notice that you pass two arguments to this method, the\nform\nobject and the URL of the\nlogin_page\n, which you access via\nlogin_page.url\n.\nIn the interactive window, you confirm that the submission successfully redirected to the\n/profiles\npage. If something had gone wrong, then the value of\nprofiles_page.url\nwould still be\n\"http://olympus.realpython.org/login\"\n.\nNote:\nHackers can use automated programs like the one above to\nbrute force\nlogins by rapidly trying many different usernames and passwords until they find a working combination.\nBesides this being highly illegal, almost all websites these days lock you out and report your IP address if they see you making too many failed requests, so don\u2019t try it!\nNow that you have the\nprofiles_page\nvariable set, it\u2019s time to programmatically obtain the URL for each link on the\n/profiles\npage.\nTo do this, you use\n.select()\nagain, this time passing the string\n\"a\"\nto select all the\n<a>\nanchor elements on the page:\nPython\n>>>\nlinks\n=\nprofiles_page\n.\nsoup\n.\nselect\n(\n\"a\"\n)\nNow you can iterate over each link and print the\nhref\nattribute:\nPython\n>>>\nfor\nlink\nin\nlinks\n:\n...\naddress\n=\nlink\n[\n\"href\"\n]\n...\ntext\n=\nlink\n.\ntext\n...\nprint\n(\nf\n\"\n{\ntext\n}\n:\n{\naddress\n}\n\"\n)\n...\nAphrodite: /profiles/aphrodite\nPoseidon: /profiles/poseidon\nDionysus: /profiles/dionysus\nThe URLs contained in each\nhref\nattribute are relative URLs, which aren\u2019t very helpful if you want to navigate to them later using MechanicalSoup. If you happen to know the full URL, then you can assign the portion needed to construct a full URL.\nIn this case, the base URL is just\nhttp://olympus.realpython.org\n. Then you can concatenate the base URL with the relative URLs found in the\nsrc\nattribute:\nPython\n>>>\nbase_url\n=\n\"http://olympus.realpython.org\"\n>>>\nfor\nlink\nin\nlinks\n:\n...\naddress\n=\nbase_url\n+\nlink\n[\n\"href\"\n]\n...\ntext\n=\nlink\n.\ntext\n...\nprint\n(\nf\n\"\n{\ntext\n}\n:\n{\naddress\n}\n\"\n)\n...\nAphrodite: http://olympus.realpython.org/profiles/aphrodite\nPoseidon: http://olympus.realpython.org/profiles/poseidon\nDionysus: http://olympus.realpython.org/profiles/dionysus\nYou can do a lot with just\n.get()\n,\n.select()\n, and\n.submit()\n. That said, MechanicalSoup is capable of much more. To learn more about MechanicalSoup, check out the\nofficial docs\n.\nRemove ads\nCheck Your Understanding\nExpand the block below to check your understanding\nExercise: Submit a Form With MechanicalSoup\nShow/Hide\nUse MechanicalSoup to provide the correct username (\nzeus\n) and password (\nThunderDude\n) to the\nlogin form\nlocated at the URL\nhttp://olympus.realpython.org/login\n.\nOnce the form is submitted, display the title of the current page to determine that you\u2019ve been redirected to the\n/profiles\npage.\nYour program should print the text\n<title>All Profiles</title>\n.\nYou can expand the block below to see a solution.\nSolution: Submit a Form With MechanicalSoup\nShow/Hide\nFirst, import the\nmechanicalsoup\npackage and create a\nBroswer\nobject:\nPython\nimport\nmechanicalsoup\nbrowser\n=\nmechanicalsoup\n.\nBrowser\n()\nPoint the browser to the login page by passing the URL to\nbrowser.get()\nand grab the HTML with the\n.soup\nattribute:\nPython\nlogin_url\n=\n\"http://olympus.realpython.org/login\"\nlogin_page\n=\nbrowser\n.\nget\n(\nlogin_url\n)\nlogin_html\n=\nlogin_page\n.\nsoup\nlogin_html\nis a\nBeautifulSoup\ninstance. Because the page has only a single form on it, you can access the form via\nlogin_html.form\n. Using\n.select()\n, select the username and password inputs and fill them with the username\n\"zeus\"\nand the password\n\"ThunderDude\"\n:\nPython\nform\n=\nlogin_html\n.\nform\nform\n.\nselect\n(\n\"input\"\n)[\n0\n][\n\"value\"\n]\n=\n\"zeus\"\nform\n.\nselect\n(\n\"input\"\n)[\n1\n][\n\"value\"\n]\n=\n\"ThunderDude\"\nNow that the form is filled out, you can submit it with\nbrowser.submit()\n:\nPython\nprofiles_page\n=\nbrowser\n.\nsubmit\n(\nform\n,\nlogin_page\n.\nurl\n)\nIf you filled the form with the correct username and password, then\nprofiles_page\nshould actually point to the\n/profiles\npage. You can confirm this by printing the title of the page assigned to\nprofiles_page:\nPython\nprint\n(\nprofiles_page\n.\nsoup\n.\ntitle\n)\nYou should see the following text displayed:\nShell\n<title>All Profiles</title>\nIf instead you see the text\nLog In\nor something else, then the form submission failed.\nWhen you\u2019re ready, you can move on to the next section.\nInteract With Websites in Real Time\nSometimes you want to be able to fetch real-time data from a website that offers continually updated information.\nIn the dark days before you learned Python programming, you had to sit in front of a browser, clicking the\nRefresh\nbutton to reload the page each time you wanted to check if updated content was available. But now you can automate this process using the\n.get()\nmethod of the MechanicalSoup\nBrowser\nobject.\nOpen your browser of choice and navigate to the URL\nhttp://olympus.realpython.org/dice\n:\nThis\n/dice\npage simulates a roll of a six-sided die, updating the result each time you refresh the browser. Below, you\u2019ll write a program that repeatedly scrapes the page for a new result.\nThe first thing you need to do is determine which element on the page contains the result of the die roll. Do this now by right-clicking anywhere on the page and selecting\nView page source\n. A little more than halfway down the HTML code is an\n<h2>\ntag that looks like this:\nHTML\n<\nh2\nid\n=\n\"result\"\n>\n3\n</\nh2\n>\nThe text of the\n<h2>\ntag might be different for you, but this is the page element you need for scraping the result.\nNote:\nFor this example, you can easily check that there\u2019s only one element on the page with\nid=\"result\"\n. Although the\nid\nattribute is supposed to be unique, in practice you should always check that the element you\u2019re interested in is uniquely identified.\nNow start by writing a simple program that opens the\n/dice\npage, scrapes the result, and prints it to the console:\nPython\nmech_soup.py\nimport\nmechanicalsoup\nbrowser\n=\nmechanicalsoup\n.\nBrowser\n()\npage\n=\nbrowser\n.\nget\n(\n\"http://olympus.realpython.org/dice\"\n)\ntag\n=\npage\n.\nsoup\n.\nselect\n(\n\"#result\"\n)[\n0\n]\nresult\n=\ntag\n.\ntext\nprint\n(\nf\n\"The result of your dice roll is:\n{\nresult\n}\n\"\n)\nThis example uses the\nBeautifulSoup\nobject\u2019s\n.select()\nmethod to find the element with\nid=result\n. The string\n\"#result\"\n, which you pass to\n.select()\n, uses the\nCSS ID selector\n#\nto indicate that\nresult\nis an\nid\nvalue.\nTo periodically get a new result, you\u2019ll need to create a loop that loads the page at each step. So everything below the line\nbrowser = mechanicalsoup.Browser()\nin the above code needs to go in the body of the loop.\nFor this example, you want four rolls of the dice at ten-second intervals. To do that, the last line of your code needs to tell Python to pause running for ten seconds. You can do this with\n.sleep()\nfrom Python\u2019s\ntime\nmodule\n. The\n.sleep()\nmethod takes a single argument that represents the amount of time to sleep in seconds.\nHere\u2019s an example that illustrates how\nsleep()\nworks:\nPython\nimport\ntime\nprint\n(\n\"I'm about to wait for five seconds...\"\n)\ntime\n.\nsleep\n(\n5\n)\nprint\n(\n\"Done waiting!\"\n)\nWhen you run this code, you\u2019ll see that the\n\"Done waiting!\"\nmessage isn\u2019t displayed until five seconds have passed from when the first\nprint()\nfunction was executed.\nFor the die roll example, you\u2019ll need to pass the number\n10\nto\nsleep()\n. Here\u2019s the updated program:\nPython\nmech_soup.py\nimport\ntime\nimport\nmechanicalsoup\nbrowser\n=\nmechanicalsoup\n.\nBrowser\n()\nfor\ni\nin\nrange\n(\n4\n):\npage\n=\nbrowser\n.\nget\n(\n\"http://olympus.realpython.org/dice\"\n)\ntag\n=\npage\n.\nsoup\n.\nselect\n(\n\"#result\"\n)[\n0\n]\nresult\n=\ntag\n.\ntext\nprint\n(\nf\n\"The result of your dice roll is:\n{\nresult\n}\n\"\n)\ntime\n.\nsleep\n(\n10\n)\nWhen you run the program, you\u2019ll immediately see the first result printed to the console. After ten seconds, the second result is displayed, then the third, and finally the fourth. What happens after the fourth result is printed?\nThe program continues running for another ten seconds before it finally stops. That\u2019s kind of a waste of time! You can stop it from doing this by using an\nif\nstatement\nto run\ntime.sleep()\nfor only the first three requests:\nPython\nmech_soup.py\nimport\ntime\nimport\nmechanicalsoup\nbrowser\n=\nmechanicalsoup\n.\nBrowser\n()\nfor\ni\nin\nrange\n(\n4\n):\npage\n=\nbrowser\n.\nget\n(\n\"http://olympus.realpython.org/dice\"\n)\ntag\n=\npage\n.\nsoup\n.\nselect\n(\n\"#result\"\n)[\n0\n]\nresult\n=\ntag\n.\ntext\nprint\n(\nf\n\"The result of your dice roll is:\n{\nresult\n}\n\"\n)\n# Wait 10 seconds if this isn't the last request\nif\ni\n<\n3\n:\ntime\n.\nsleep\n(\n10\n)\nWith techniques like this, you can scrape data from websites that periodically update their data. However, you should be aware that requesting a page multiple times in rapid succession can be seen as suspicious, or even malicious, use of a website.\nImportant:\nMost websites publish a Terms of Use document. You can often find a link to it in the website\u2019s footer.\nAlways read this document before attempting to scrape data from a website. If you can\u2019t find the Terms of Use, then try to contact the website owner and ask them if they have any policies regarding request volume.\nFailure to comply with the Terms of Use could result in your IP being blocked, so be careful!\nIt\u2019s even possible to crash a server with an excessive number of requests, so you can imagine that many websites are concerned about the volume of requests to their server! Always check the Terms of Use and be respectful when sending multiple requests to a website.\nRemove ads\nConclusion\nAlthough it\u2019s possible to parse data from the Web using tools in Python\u2019s standard library, there are many tools on PyPI that can help simplify the process.\nIn this tutorial, you learned how to:\nRequest a web page using Python\u2019s built-in\nurllib\nmodule\nParse HTML using\nBeautiful Soup\nInteract with web forms using\nMechanicalSoup\nRepeatedly request data from a website to\ncheck for updates\nWriting automated web scraping programs is fun, and the Internet has no shortage of content that can lead to all sorts of exciting projects.\nJust remember, not everyone wants you pulling data from their web servers. Always check a website\u2019s Terms of Use before you start scraping, and be respectful about how you time your web requests so that you don\u2019t flood a server with traffic.\nSource Code:\nClick here to download the free source code\nthat you\u2019ll use to collect and parse data from the Web.\nAdditional Resources\nFor more information on web scraping with Python, check out the following resources:\nBeautiful Soup: Build a Web Scraper With Python\nAPI Integration in Python\nPython & APIs: A Winning Combo for Reading Public Data\nNote:\nIf you enjoyed what you learned in this sample from\nPython Basics: A Practical Introduction to Python 3\n, then be sure to check out\nthe rest of the book\n.\nTake the Quiz:\nTest your knowledge with our interactive \u201cA Practical Introduction to Web Scraping in Python\u201d quiz. You\u2019ll receive a score upon completion to help you track your learning progress:\nInteractive Quiz\nA Practical Introduction to Web Scraping in Python\nIn this quiz, you'll test your understanding of web scraping in Python. Web scraping is a powerful tool for data collection and analysis. By working through this quiz, you'll revisit how to parse website data using string methods, regular expressions, and HTML parsers, as well as how to interact with forms and other website components.\nFrequently Asked Questions\nNow that you have some experience with web scraping in Python, you can use the questions and answers below to check your understanding and recap what you\u2019ve learned.\nThese FAQs are related to the most important concepts you\u2019ve covered in this tutorial. Click the\nShow/Hide\ntoggle beside each question to reveal the answer.\nIs Python good for web scraping?\nShow/Hide\nYes, Python is a popular choice for web scraping due to its ease of use and the availability of powerful libraries like Beautiful Soup and MechanicalSoup that simplify the process.\nHow can you scrape websites with Python?\nShow/Hide\nYou can scrape websites with Python by using libraries like\nurllib\nto fetch HTML, Beautiful Soup to parse HTML, and MechanicalSoup to interact with web forms.\nIs data scraping illegal?\nShow/Hide\nData scraping can be illegal if it violates a website\u2019s terms of service or involves accessing data without permission. Always check the website\u2019s acceptable use policy before scraping.\nWhat tools can you use for parsing HTML in Python?\nShow/Hide\nYou can use tools such as Beautiful Soup and\nlxml\nto parse HTML in Python. These libraries make it easy to navigate and extract data from HTML documents.\nHow can you handle forms in web scraping?\nShow/Hide\nYou can handle forms in web scraping using MechanicalSoup, which allows you to fill out and submit forms programmatically within a headless browser session.\nMark as Completed\nShare\nWatch Now\nThis tutorial has a related video course created by the Real Python team. Watch it together with the written tutorial to deepen your understanding:\nIntroduction to Web Scraping With Python\n\ud83d\udc0d Python Tricks \ud83d\udc8c\nGet a short & sweet\nPython Trick\ndelivered to your inbox every couple of days. No spam ever. Unsubscribe any time. Curated by the Real Python team.\nSend Me Python Tricks \u00bb\nAbout\nDavid Amos\nDavid is a writer, programmer, and mathematician passionate about exploring mathematics through code.\n\u00bb More about David\nEach tutorial at Real Python is created by a team of developers so that it meets our high quality standards. The team members who worked on this tutorial are:\nAldren\nGeir Arne\nJoanna\nJacob\nKate\nMartin\nPhilipp\nMaster\nReal-World Python Skills\nWith Unlimited Access to Real\u00a0Python\nJoin us and get access to thousands of tutorials, hands-on video courses, and a community of expert\u00a0Pythonistas:\nLevel Up Your Python Skills \u00bb\nMaster\nReal-World Python Skills\nWith Unlimited Access to Real\u00a0Python\nJoin us and get access to thousands of tutorials, hands-on video courses, and a community of expert Pythonistas:\nLevel Up Your Python Skills \u00bb\nWhat Do You Think?\nRate this article:\nLinkedIn\nTwitter\nBluesky\nFacebook\nEmail\nWhat\u2019s your #1 takeaway or favorite thing you learned? How are you going to put your newfound skills to use? Leave a comment below and let us know.\nCommenting Tips:\nThe most useful comments are those written with the goal of learning from or helping out other students.\nGet tips for asking good questions\nand\nget answers to common questions in our support portal\n.\nLooking for a real-time conversation? Visit the\nReal Python Community Chat\nor join the next\n\u201cOffice\u00a0Hours\u201d Live Q&A Session\n. Happy Pythoning!\nKeep Learning\nRelated Topics:\nintermediate\nweb-scraping\nRecommended Video Course:\nIntroduction to Web Scraping With Python\nRelated Tutorials:\nBeautiful Soup: Build a Web Scraper With Python\nModern Web Automation With Python and Selenium\nPython & APIs: A Winning Combo for Reading Public Data\nWeb Scraping With Scrapy and MongoDB\nHow to Download Files From URLs With Python\nKeep reading Real\u00a0Python by creating a free account or signing\u00a0in:\nContinue \u00bb\nAlready have an account?\nSign-In\nAlmost there! Complete this form and click the button below to gain instant\u00a0access:\n\u00d7\nA Practical Introduction to Web Scraping in Python (Source Code)\nSend Code \u00bb\n\ud83d\udd12 No spam. We take your privacy seriously.",
        "image_urls": [
          {
            "url": "https://files.realpython.com/media/Python-Basics-Chapter-on-Web-Scraping_Watermarked.f8d56f56c22c.jpg",
            "score": 2
          },
          {
            "url": "https://files.realpython.com/media/Python-Basics-Chapter-on-Web-Scraping_Watermarked.f8d56f56c22c.jpg",
            "score": 2
          },
          {
            "url": "https://files.realpython.com/media/website_aphrodite.10b67047ebc2.png",
            "score": 2
          },
          {
            "url": "https://files.realpython.com/media/website_dionysos_page.8d7be251d9a0.png",
            "score": 2
          },
          {
            "url": "https://files.realpython.com/media/website_login.739f488fbe74.png",
            "score": 2
          },
          {
            "url": "https://files.realpython.com/media/website_dice.3cdd09061f55.png",
            "score": 2
          },
          {
            "url": "https://files.realpython.com/media/Python-Basics-Chapter-on-Web-Scraping_Watermarked.f8d56f56c22c.jpg",
            "score": 2
          },
          {
            "url": "https://realpython.com/cdn-cgi/image/width=800,height=800,fit=crop,gravity=auto,format=auto/https://files.realpython.com/media/gahjelle.470149ee709e.jpg",
            "score": 2
          },
          {
            "url": "https://realpython.com/cdn-cgi/image/width=800,height=800,fit=crop,gravity=auto,format=auto/https://files.realpython.com/media/jjablonksi-avatar.e37c4f83308e.jpg",
            "score": 2
          },
          {
            "url": "https://files.realpython.com/media/Python-Basics-Chapter-on-Web-Scraping_Watermarked.f8d56f56c22c.jpg",
            "score": 2
          }
        ],
        "title": "A Practical Introduction to Web Scraping in Python \u2013 Real Python"
      },
      {
        "url": "https://www.youtube.com/watch?v=DcI_AZqfZVc",
        "raw_content": "Advanced Web Scraping Tutorial! (w/ Python Beautiful Soup Library) - YouTube\nAbout\nPress\nCopyright\nContact us\nCreators\nAdvertise\nDevelopers\nTerms\nPrivacy\nPolicy & Safety\nHow YouTube works\nTest new features\nNFL Sunday Ticket\n\u00a9 2025 Google LLC",
        "image_urls": [],
        "title": "Advanced Web Scraping Tutorial! (w/ Python Beautiful Soup Library) - YouTube"
      }
    ]
  },
  "duckduckgo_Technology News": {
    "success": false,
    "error": "Unable to import ddgs. Please install with `pip install -U ddgs`"
  },
  "duckduckgo_Climate Research": {
    "success": false,
    "error": "Unable to import ddgs. Please install with `pip install -U ddgs`"
  },
  "duckduckgo_Programming Tutorials": {
    "success": false,
    "error": "Unable to import ddgs. Please install with `pip install -U ddgs`"
  }
}